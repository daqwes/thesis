\documentclass[12pt]{memoir}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{3} 
\usepackage[dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{url}
\usepackage{pdfpages}
\raggedbottom
% \usepackage{biblatex}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{bibliography.bib}

% Images and figure packages
\usepackage{float}
\usepackage{svg}
\usepackage{subcaption}

% Math packages
\usepackage{amssymb}
%\usepackage[cal=boondoxo]{mathalfa}
\usepackage{amsmath}
\usepackage{empheq}
\usepackage{thmtools}
\usepackage{mathtools}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\declaretheorem[sibling=definition, shaded={rulecolor=black, rulewidth=0.6pt, bgcolor={rgb}{1,1,1}},name=Definition]{boxeddef}
\declaretheorem[sibling=theorem, shaded={rulecolor=black, rulewidth=0.6pt, bgcolor={rgb}{1,1,1}},name=Theorem]{boxedthm}



\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=Green,
    pdftitle={Thesis - Danila Mokeev},
    pdfpagemode=FullScreen,
}
\usepackage{underscore}
\usepackage{braket}
\usepackage{afterpage}

% algorithms
\usepackage[linesnumbered,ruled]{algorithm2e}

\setlength\parindent{0pt}

% Commands/shortcuts
% example def of command
% \newcommand{\plusbinomial}[3][2]{(#2 + #3)^#1}
% 3 parameters, with 2 as a default value for #1
\newcommand{\tr}{\text{tr}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\mb}{\mathbf}
\newcommand{\tb}{\textbf}
\newcommand{\ti}{\textit}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\brho}{\boldsymbol{\rho}}

\title{Efficient sampling algorithms for Bayesian Quantum Tomography}
\author{Danila Mokeev}
\date{June 2024}

\begin{document}
\includepdf[pages={1}]{cover/cover-danila-mokeev.pdf}
\maketitle


\newpage

\chapter*{Acknowledgements}
I would like to thank Estelle Massart, Andrew Thompson and Tameem Adel for their help in the making of this thesis. Their guidance and great ideas were valuable for the . I also want to thank Matthieu Génévriez for his help on making the Quantum Tomography section more rigorous, and researchers Pierre Alquier and The Tien Mai for providing the source code for the prob-estimator. Finally, I want to thank my family who supported me throughout the university years.
\newpage

\tableofcontents*

\newpage

\chapter*{Abbreviations and symbols}
\addcontentsline{toc}{chapter}{\protect\numberline{}Abbreviations and symbols}
\begin{tabular}{ll}
\tb{MCMC} & Markov chain Monte Carlo\\
\tb{prob} & prob-estimator\\
\tb{PL}& Projective Langevin  \\
$\ket{{\psi}}$ & Quantum state $\psi$ in the \ti{bra-ket} formulation\\
$\mb{M^\dagger}$ & Conjugate transponse of matrix $M$\\
$\braket{\mb M}$ & Expected value of a matrix/operator $M$\\
\tb {POVM} & Positive Operator-Valued Measure\\
$\mb{M\otimes N}$ & Kronecker product between matrices $M$ and $N$\\
${U}(a,b)$ & Uniform distribution defined on the open interval $(a,b)$\\
$N(\mu, \sigma^2)$ & Normal distribution with mean $\mu$ and standard derivation $\sigma$\\
$N(\boldsymbol{\mu},\boldsymbol{\Sigma})$ & Multivariate normal distribution with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$ 
\end{tabular}

\chapter{Introduction}

\section{Motivation}
%\addcontentsline{toc}{chapter}{\protect\numberline{}Motivation}
A fundamental problem in physics is the reconstruction of the state of a system given measurements describing it. In classical mechanics, this is, at least in theory, always possible due to the deterministic nature of the system. This allows one to make multiple measurements on a system, usually the position and momentum, to exactly determine its state. In Quantum Mechanics (Quantum Mechanics), however, the situation is very different: measuring the system disturbs it (in Quantum Mechanics we talk about wavefunction collapse), making it impossible to gain more information by repeating measurements. We are also fundamentally limited by what we can measure: the Heisenberg uncertainty principle only allows the measurement of either the position or the momentum, but not both at the same time. Finally, the no-cloning theorem also forbids the system to be copied right before the measurement unless we know the state of the system.\medbreak

\textit{Quantum Tomography} or \textit{Quantum State Tomography} is a process that allows to reconstruct the state of the system in the context of Quantum Mechanics. It tackles the previously described issues by replicating the initial state of the system multiple times, and then measuring once each replica. Note that there is a subtle difference between copying the system and preparing multiple systems with the same steps, as the latter does \textit{not} require any knowledge about the state, only the \textit{steps} necessary to produce it.\medbreak

The term ``tomography" means ``to describe an object based on sections, slices" \cite{wiki:tomography}. Many types of tomographies exist and are used in very diverse environments, with notable examples in medical applications, such as Computed Tomography (CT) or Magnetic Resonance Imaging (MRI). Quantum Tomography works in a similar way: we use a set of slices (observables), each of them probing a particular aspect of that state, to reconstruct the quantum state through a tomographic process. An intuitive analogy is the process of shining light on each side of a 3D object, and reconstructing that object from the obtained shadows. This results in an inverse problem, whereby starting from partial and noisy data (and in this case fundamentally probabilistic), we can obtain the true state of the system.


\section{Quantum Tomography} \label{introduction:quantum-tomography}

%To quote Wikipedia \cite{wiki:Quantum Tomography}: ``Quantum tomography or quantum state tomography is the process by which a quantum state is reconstructed using measurements on an ensemble of identical quantum states". It is therefore an inverse problem, whereby starting from noisy data, we can reconstruct the true state of the system.\\\\
As previously described, the goal of Quantum Tomography is the reconstruction of the system state based on measurements. All of this system information is stored in a matrix, called the \textit{density matrix} $\rho \in \mathbb{C}^{d \times d}$, with $d = 2^n$ and $n$ the number of qubits. Due to the physical properties of the system, this matrix is Hermitian ($\rho = \rho^{\dagger}$), positive semi-definite ($\rho > 0$) and satisfying $\tr(\rho) = 1$. The rank of the matrix also has importance, as it conveys a physical property of the system: pure states are of rank 1, while mixed states are of any rank between 2 and $d$. A pure state is described by the wavefunction, which is usually represented by a vector, potentially of infinite dimension. Mixed states, on the other hand, are a generalization of pure states and are represented by a density matrix. They correspond to a probabilistic mixture of pure states. Pure states correspond to matrices of rank 1, while mixed states are of rank 2 and larger. It is important to note that physicists are mostly interested in density matrices of low rank.\medbreak

Quantum Tomography is based on Born's rule, which links a projective measurement $P_m$ associated to an \textit{observable} $O$ to the probability $p(m)$ of obtaining the associated eigenvalue $m$ given a density matrix $\rho$. This can generally be written as 
\begin{equation}
    p(m) = \tr(\rho P_m)
\end{equation}
By repeating the measurement on many different but identically prepared systems for each observable, we are able to obtain statistically significant estimates for $p(m)$, which combined with the projectors $P_m$, allows us to estimate the density matrix with any appropriate method.
\section{Overview of existing methods}
We first provide in \ref{table:methods-comp} a summary of existing methods with the papers using them, followed by a more detailed explanation below.  

\begin{table}[H]

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        Method & Papers \\\hline\hline
        Inversion & \cite{meth:linear-inversion:vogel-risken, meth:linear-inversion:RMH, meth:linear-inversion:alquier} \\\hline
        Pauli basis expansion & \cite{Cai-2016}\\\hline
        Maximum Likelihood & \cite{Guta20,meth:ML:BDP,meth:ML:JKMW,meth:ML:Lvovsky,meth:ML:Blume-Kohout,meth:ML:Suzuki,meth:ML:Hradil2004} \\\hline
        Compressed Sensing & \cite{meth:CS:GLFSBE10,meth:CS:Gross-2011, meth:CS:Flammia-2012,meth:CS:Koltchinskii-2011}\\\hline
        Sampling based & \cite{meth:bayesian:smc:Ferrie-2014,meth:bayesian:smc:Kueng-2015, meth:bayesian:smc:Granade_2016,meth:bayesian:sis:Kravtsov-2013}\\\hline
        Metropolis-Hastings based & \cite{MA17,Mai22,LLJL20, meth:bayesian:mh:Blume-Kohout-2010}\\\hline
        Langevin & \cite{meth:bayesian:Langevin:ACMT2024}\\\hline
    \end{tabular}
\end{center}
\caption{Overview of existing methods with associated papers}
\label{table:methods-comp}
\end{table}


Well-established methods exist to approximate $\rho$. We can split the existing methods into 5 main categories: inversion, Pauli basis expansion, Maximum Likelihood, Compressed sensing and finally Bayesian methods, which are mostly sampling-based algorithms.\medbreak

Inversion methods are very simple, as they directly rely on the Born rule: $\hat \rho$ is computed by solving the linear system of equations defined by 
\begin{equation*} \label{eq:inversion-method}
        \forall a \in \mathcal{A},\forall s \in \mathcal{S}: \, \hat p_{a,s} = \tr(\hat \rho P^a_s)
\end{equation*}
where $\mathcal{A}$ denotes the set of indices for observables, $\mathcal{S}$ the set of indices for projective measurements, $\hat p_{a,s}$ the empirical probability and $P^a_s$ the projection operator. This approach was introduced in \cite{meth:linear-inversion:vogel-risken}, and then used in \cite{meth:linear-inversion:RMH}, but also in \cite{meth:linear-inversion:alquier}, where they additionally use a rank penalization term. This method is easy to understand, however, it returns a matrix $\hat \rho$ which does not satisfy the properties of a density matrix.\medbreak
%and requires the set of observables to be tomographically complete, which is usually the case but limits us, for example, to run any ablation studies related to the number of observables.
% \begin{equation}
%     \hat p_{a,s} = \tr(\hat \rho \cdot P^{a,s})
% \end{equation}
A second approach, introduced in \cite{Cai-2016}, works by estimating the coefficients in the Pauli basis expansion. Indeed, as the observables of interest we use in Quantum Tomography are the Kronecker product of Pauli matrices (which are usually called $\{\sigma_x, \sigma_y, \sigma_z\} \in \mathbb{C}^{2\times 2}$), we can also approximate $\rho$ using the resulting observable as an element of a base. If we have $\mathcal{B} = \{\sigma_b = \sigma_{b_1} \otimes \dots \otimes \sigma_{b_n}, b \in \{I,x,y,z\}^n\}$, then the density matrix $\rho$ can be approximated as 
\begin{equation}
    \rho = \sum_{b\in\{I,x,y,z\}^n} \rho_b \sigma_b
\end{equation}
In case of \cite{Cai-2016}, the coefficients $\rho_b$ are estimated using an average of the measured eigenvalues with each observable.\medbreak

Another common approach used by many \cite{Guta20,meth:ML:BDP,meth:ML:JKMW,meth:ML:Lvovsky,meth:ML:Blume-Kohout,meth:ML:Suzuki,meth:ML:Hradil2004} is Maximum Likelihood (ML) estimation, which tries to maximize an objective function $\mathcal{L}$ to find the best $\rho$ which matches the data. A possible loss function could be 
\begin{equation}
\mathcal{L}(\rho ; \mb{D}) \propto \prod_{a \in \mathcal{A}} \prod_{s \in \mathcal{S}}\left[\tr\left(\rho P_{{s}}^{{a}}\right)\right]^{n_{{a}, {s}}},
\end{equation}
where $n_{a,s}$ is the number of occurences of $s$ in measurements by $a$. The density matrix is calculated as 
\begin{equation}
    \hat \rho_{\text{ML}} = \text{argmin}_{\rho} \;\mathcal{L}(\rho;\mb{D})
\end{equation}
A downside however often mentioned for this method is the cost, notably for $n\geq10$
\medbreak

There is also a certain number of papers \cite{meth:CS:GLFSBE10,meth:CS:Gross-2011, meth:CS:Flammia-2012,meth:CS:Koltchinskii-2011} which treat the problem of solving \ref{eq:inversion-method} as a pure optimization problem, with the extra contraint that not all observables are available (compressed sensing). They are however less relevant to us, as we consider to be in the tomographically complete case.\medbreak

% Based on the Born rule, we can write the problem as $p_{i} = \tr(\rho \cdot P_i)$ where $p_i$ corresponds to the probability of obtaining $P_i$. The notation $P_i$ refers here to an observable, meaning that it will measure the system at a particular state. This can for example be the spin of each qubit (up or down, in either direction \{x,y,z\}).\\\\ 
% The problem is then framed as follows: based on the empirical frequencies $\hat p_i$ and the observables, which are both known, estimate the density matrix $\rho$ (which we call $\hat \rho$). The empirical probabilities $\hat p_i$ are estimated using $m$ independent copies of the system measured with $P_i$. 
% We will usually refer to $m$ as the number of shots.\\\\

% \[
% \hat p_{\mathbf{a,s}} = \cfrac{1}{m}\sum_{i=1}^m \mathbf{1}_{\{R^\mathbf{a}_i = \mathbf{s}\}}
% \]
% \[
% \hat p_{i} = \cfrac{1}{m}\sum_{i=1}^m \mathbf{1}_{}
% \]
% \[
% \hat p_{\mathbf{a,s}} = \tr(\hat \rho \cdot P^{\mb a}_{\mb s})
% \]
% \[
% \hat p_{i} = \tr(\hat \rho \cdot P_i)
% \]

The methods described so far do not make any assumptions about the structure of $\rho$. They may enforce the properties of a density matrix, but not use the low-rank information, with the only exception being papers which include a rank regularization term in the loss function (such as \cite{meth:linear-inversion:alquier}). To tackle this issue, a good idea is to venture into Bayesian methods, which naturally define a \textit{prior} on $\rho$. They also have the advantage of providing a distribution over it, rather than a point estimate, as most optimization-based do.\medbreak

Bayesian methods rely on \ti{Bayes} theorem, which states that given data $\mb{D}$, which in our case are the empirical probabilities $p_{a,s}$ and projectors $P^a_s$, and parameters $\theta$, in our case the density matrix $\rho$, we can calculate the posterior distribution $\pi(\rho|\mb{D})$:
\begin{equation}
    \pi(\rho|\mb{D}) \propto \pi(\mb{D}|\rho) \pi(\rho)
\end{equation}
where $Pr(\mb{D}|\rho)$ is the likelihood of $\mb{D}$ given $\rho$ and $\pi(\rho)$ the prior distribution over it. There is also a normalizing term, however it normally omitted as it does not modify the result. Obtaining samples from this posterior is what allows the approximation, and the algorithm used for sampling is the key part in the various approaches. The most common estimator then used is the sample mean, also sometimes called the \ti{Bayesian Mean Estimator} (BME) or the Gibbs estimator:
\begin{equation}
    \hat \rho_{\text{BME}} = \int \hat \rho \pi(\hat \rho|\mathcal D) d\hat \rho
\end{equation}
Markov Chain Monte Carlo (MCMC) methods are a big class of sampling methods and are the common tool of choice. In particular, \cite{MA17,Mai22,LLJL20, meth:bayesian:mh:Blume-Kohout-2010} all propose algorithms derived from the very classical Metropolis-Hastings, an algorithm that performs a random walk from an initial point, with an acceptance/rejection step. There are also other strategies, such as Sequential Monte Carlo (SMC) \cite{meth:bayesian:smc:Ferrie-2014,meth:bayesian:smc:Kueng-2015, meth:bayesian:smc:Granade_2016} or Sequential Importance Sampling \cite{meth:bayesian:sis:Kravtsov-2013}. Finally, there is also a new method introduced in \cite{meth:bayesian:Langevin:ACMT2024}, where the authors use the gradient information from the posterior, resulting in Langevin Sampling. This technique provides faster convergence times.\medbreak

The work in the thesis will focus on the prob-estimator introduced in \cite{MA17}, as well as the projected Langevin approach introduced in \cite{meth:bayesian:Langevin:ACMT2024}. They differ in the algorithms they use, Metropolis-Hastings and Langevin sampling, but also in the prior they choose. A complete description of both of these methods is available in section \ref{section:relevant-methods}.

% These methods however have drawbacks, mainly their generic structure, not allowing adding extra information about $\rho$. The inversion method also returns a matrix $\hat \rho$ which does not satisfy the properties of a density matrix, and requires the set of observables to be tomographically complete, which is usually is the case but limits us, for example, to run any ablation studies related to the number of observables.

% This is when the Bayesian framework comes into play: by formulating our problem as 
% $$p(\rho|D) \propto p(D|\rho) p(\rho)$$
% we can add the extra information, for example the fact that $rho$ is low rank. Here, $p(\rho|D)$ is the posterior (distribution of $\rho$ given the data), $p(D|\rho)$ the likelihood and $p(\rho)$ the prior.


% They can either be computed on real hardware by replicating the system $m$ times and measuring with each observable on every copy, or by generating the data. The former requires access to a quantum computer, and for the sake of experiments that will be done, the latter is often enough.


%\section{Markov chain Monte Carlo methods}


\section{Goals and contributions}\label{section:goals-contributions}
In this thesis, our contribution is twofold: first, we investigate how the prob-estimator introduced in \cite{MA17} compares to the projected Langevin algorithm from \cite{meth:bayesian:Langevin:ACMT2024}; second, we try to understand how much impact the used prior has in comparison to the MCMC algorithm that we use to perform the sampling. For the latter, we introduce 2 new algorithms, Metropolis-Hastings with Student-t prior (MHS) and Metropolis-Hastings with Gibbs with Student-t prior (MHGS), which mix the algorithm from \cite{MA17} with the prior from \cite{meth:bayesian:Langevin:ACMT2024}. This allows us to evaluate the advantages that a gradient-based method brings, as well as the effect of a student-t prior on the result. We will focus on providing numerical results, with all the code to reproduce the experiments available in the near future on Github\footnote{\texttt{\url{www.github.com/daqwes/thesis}}}.

\section{Structure}
This thesis will be structured as follows.

In Chapter 2, we will provide the needed background to understand Quantum Tomography in the context of numerical experiments, as well as an introduction to Markov chain Monte Carlo methods, with an emphasis on the methods relevant for this thesis.\medbreak

In Chapter 3, we will review in detail the 2 main algorithms used in the numerical experiments in the context of this work: the prob-estimator and the Projected Langevin algorithm.\medbreak

In Chapter 4, we will numerically compare both of the algorithms in terms of accuracy in different experimental setups.\medbreak

Finally, in Chapter 5, we will introduce 2 new algorithms, MHS and MHGS, to better understand how the choice of the algorithm and prior impacts the accuracy. This will allow us to shine light on the potential benefits of the gradient information and the student-t prior, brought by the Projected Langevin method.


\chapter{Background}

\section{Quantum information}

Quantum information is a field that explores how information can be encoded and processed using the principles of quantum mechanics. While classical computers perform operations using \textit{bits} (hence either $0$ or $1$), quantum computers operate on $qubits$, which allows for a superposition of $0$ and $1$. This means that a \textit{quantum state} can be in an infinite combination between $0$ and $1$. This property is more clearly explained using probabilities, where the likelihood of measuring the qubit in a certain state will be proportional to the norm squared of the coefficients of the superposition. It is important to note that when we perform the measurement, only one of the states, $0$ or $1$, will be visible.\medbreak

This measurement process is very important in quantum mechanics, as contrary to classical mechanics, we can not simply observe the system. There are 4 main properties that limit us: fundamentally, a quantum system is probabilistic - measuring it once does not give complete information about the state; in addition, a measurement makes the system wavefunction collapse - repeating the measurement only yields the same state again; the no-cloning theorem, which does not allow to simply copy the state; finally, the Heisenberg uncertainty principle, which states that either component of certain pairs of conjugate operators (such as the position and momentum, for example) can be measured, but not both. Given that, we have to find ways to measure the quantum state of the system, and this is where Quantum Tomography fits into the picture.\medbreak


\section{The qubit}
The $qubit$ is the fundamental unit of computation in quantum information. It contrasts from classical bits by allowing a continuum of possible states: a linear combination between $0$ and $1$. We usually use the orthonormal basis states with the \textit{bra-ket} formulation to represent it: $\ket 0 = \begin{bmatrix}
    1\\
    0
\end{bmatrix}$ and $\ket 1 = \begin{bmatrix}
    0\\
    1
\end{bmatrix}$. The resulting state $\psi$ is written as follows:
\begin{equation}
    \psi = \alpha \ket 0 + \beta \ket 1 
\end{equation}
where $\alpha,\beta \in \mathbb{C}$ are probability amplitudes, with the constraint that $|\alpha|^2 + |\beta|^2 = 1$. This last equality is needed to satisfy the second axiom of probability, as after the measurement either of outcomes must be observed.\medbreak

A intuitive way to visualise a qubit is to use a \ti{Bloch sphere}. First, we must do a change of coordinates, by going from cartesian to Hopf coordinates. With this in mind, it can be shown that $\ket \psi$ can be rewritten as

\begin{equation}
    \ket \psi = e^{i\gamma}\left( \cos\left(\frac{\theta}{2}\right)\ket 0 + e^{i\phi} \sin\left(\frac{\theta}{2}\right)\ket 1\right)
\end{equation}
with $\theta,\gamma,\phi \in \mathbb{R}$. The term $e^{i\gamma}$ has no observable effects, allowing us to remove it\footnote{This is due to the fact that the projective measurement calculates the probability amplitude \textit{squared}}. This leaves us with 2 parameters $\theta$ and $\phi$, which finally allow us to represent the qubit on a sphere:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/bloch_sphere.png}
    \caption{Bloch sphere representation of a qubit \cite{wiki:qubit}}
    \label{fig:bloch-sphere}
\end{figure}
While the classical bit only exists in 2 points, the north and south poles, the qubit extends the quantum state to any position on the surface of the cube. \cite{wiki:qubit,book:Nielsen-Chuang-2010}\medbreak

The visual explanation unfortunately fades when we consider multiple qubits: in that case, it is much easier to talk about a $2^n$ dimensional basis. For example, with 2 qubits, 4 basis elements describe a quantum state:

\begin{equation}
    \ket{\phi} = \alpha \ket{00} + \beta \ket{01} + \gamma \ket{10} + \delta \ket{11}
\end{equation}

where $\alpha,\beta,\gamma,\delta \in \mathbb C$ and normalize to 1, and $\ket{\phi} \in \mathbb{C}^4$.\medbreak

\section{Quantum Tomography}
As described in \ref{introduction:quantum-tomography}, Quantum Tomography studies the reconstruction of a quantum state described by a density matrix $\rho$. It builds upon the Born rule, which states that given a system state $\ket \psi$ and an observable operator $O$, the probability of occurence of an eigenvalue $m$ of $O$ will be 
\begin{equation}
    p(m) = \braket{\psi | P_m| \psi} = \lvert\braket{m|\psi}\rvert^2
\end{equation}
where $P_m = \ket m \bra m$ is a projection onto the eigenstate of $O$ with eigenvalue $m$ (with eigenvector $\ket m$). The system is measured using $P_m$ with the outcome being an eigenstate $m$. Note that the term operator will be used interchangeably with the term matrix in what follows,as they are equivalent in this context. \medbreak

In general, one observable is however not enough as it only measures a specific aspect of a state. We will therefore create a collection of observables $O \in \mathcal{O}$, which will allow us to fully describe our state. In the situation when we have access to the entire set $\mathcal{O}$, we talk about \ti{tomographic completeness}. Of course, doing one measurement with an observable is not informative enough, and in order to obtain any statistically significant estimate for the probability of projector, we measure an ensemble $m$ of identically prepared quantum states for each $O$. \medbreak

In the following sections, we will provide a more thorough mathematical background and describe in more detail the different components that constitute Quantum Tomography.
\subsection{Mathematical description} \label{background:Quantum Tomography:math}


\subsubsection*{Pure states, mixed states and density matrix}
A \textit{pure state} is a quantum state that can be represented using a finite or infinite complex vector $\ket \psi$, element of a Hilbert space and of norm 1. Equivalently, it is a state that can not be expressed as a convex combination of other quantum states \cite{wiki:density-matrix}.\medbreak

A \textit{mixed state} $\ket \phi$, on the other hand, is a probabilistic mixture of pure states $\ket {\psi_j}$. Mixed states usually arise in situations when we do not know from which states our system state is constituted from. This means that we can not anymore represent our system state as a vector, and must resort to a more general form: the density matrix.\medbreak

A \textit{density matrix} $\rho \in \mathbb{C}^{d \times d}$, with $d = 2^n$ and $n$ the number of qubits, is a matrix that represents a general quantum system. It is written as
\begin{equation}
    \rho = \sum_{j} p_j \ket{\psi_j} \bra{\psi_j}
\end{equation}
where $p_j$ is the probability of $\ket{\psi_j}$.\medbreak

By construction, we can see that the properties of a density matrix are that it is Hermitian ($\rho = \rho^{\dagger}$), positive semi-definite ($\rho > 0$) and satisfying $\tr(\rho) = 1$. The rank of this matrix also allows to easily distinguish between a pure state of rank 1, and a mixed state of any rank between 2 and $2^n$, where $n$ is the number of qubits.

\subsubsection*{Born rule, observable and quantum measurement}

The Born rule is a fundamental postulate in Quantum Mechanics that connects the wave function to its measurement. It states that ``The probability density of finding a system in a given state, when measured, is proportional to the square of the amplitude of the system's wavefunction at that state" \cite{wiki:born-rule}.\medbreak

It is described by a collection of orthogonal projective measurements $\{P_m\}$ with $P_mP_{m'}=\delta_{mm'}P_m$. This collection relates to an \textit{observable}, represented by a Hermitian matrix $O$. This property allows it to have a spectral decomposition
\begin{equation}
    O = \sum_m m P_m
\end{equation}
where $P_m$ is the projection onto the eigenspace of $O$ associated to eigenvalue $m$. An observable corresponds to a physical property which can be measured, for example the position, momentum or spin of a particle.\medbreak

In the case where our state $\ket \psi$ is pure, the Born rule is formulated as
\begin{equation}
    p(m) = \braket{\psi|P_m|\psi}
\end{equation}
and can be further generalized to a mixed state, represented with a density matrix $\rho$
\begin{equation}\label{eq:quantum-measurement}
    p(m) = \tr(\rho P_m)
\end{equation}

A small example of such a collection are the projectors operators of the \textit{computational basis} ($\ket 0$ and $\ket 1$). The resulting matrices are $P_0 = \ket 0 \bra 0$ and $P_1 = \ket 1 \bra 1$, which indeed verify the completeness equation. If we have that $\ket \psi = a \ket 0 + b \ket 1$, then $p(0) = \braket{\psi|P_0^\dagger P_0|\psi} = \braket{\psi|P_0|\psi} = |a|^2$ (by idempotency of a projector matrix).\medbreak
    
In the context of Quantum Tomography, the most relevant aspect to measure is the spin: in each direction $\{x,y,z\}$, we use a Pauli matrix to probe whether a qubit is spin up $\ket{\cfrac{1}{2}}$ or down $\ket{-\cfrac{1}{2}}$ (eigenvalue of $1/2$ or $-1/2$)\cite{book:Nielsen-Chuang-2010,wiki:born-rule}. \medbreak

Note that, while projective measurements are the most common measurement operator, they are a specific case of the \textit{Positive Operator-Valued Measure} (POVM). This generalized measure does not enforce the orthogonality property and only requires the collection $\{F_m\}$ to be Hermitian, positive semi-definite operators on a Hilbert space, and satisfy the completeness equation
\begin{equation}
    \sum_m F_m = I
\end{equation}


\subsection{Data generation}
There are multiple ways through which one can obtain data for quantum tomography: using a real-world dataset is a possibility, however for that, you need access (or need to know someone who has) to a quantum computer. An easier approach is simply to simulate the data. It has its downsides, notably the fact that it may not be representative of true data. In our experiments, however, we will stick to synthetic data as it is very flexible and better suited for algorithm research and numerical experiments due to the low iteration time.\medbreak

When using synthetic data generation, 2 main options are usually considered: separate qubit or mixed qubit.


\subsubsection*{Separate qubit data generation}\label{section:sep-qub-dg}
Separate qubit data generation (also called the ``Pauli basis measurements" in \cite{Guta20}) is a process through which we can find the probability associated with an outcome $\mb s = (s_1, s_2 \dots, s_n) \in \mathcal{R}^n := \{-1, 1\}^n $ for an observable $\mb a = (a_1, a_2 \dots, a_n) \in \mathcal{E}^n := \{x,y,z\}^n$ (we adopt the same notations as in \cite{MA17} here).\medbreak

The observable $\sigma_{a_i}$ for a qubit can be in $\{\sigma_x, \sigma_y, \sigma_z\}$, where $\sigma_i \in \mathbb{C}^{2 \times 2}$ is one of the Pauli matrices. These matrices are written as follows:
\begin{equation}
    \sigma_x  = \begin{pmatrix}
        0 & 1\\
        1 & 0
    \end{pmatrix}\quad
    \sigma_y = \begin{pmatrix}
        0 & -i\\
        i & 0
    \end{pmatrix}\quad
    \sigma_z = \begin{pmatrix}
        1 & 0\\
        0 & -1
    \end{pmatrix}
\end{equation}
They distinguish themselves by being Hermitian, unitary and having eigenvalues $\{-1, 1\}$. They correspond to measuring the projection of the spin on the qubit, an intrinsic property of a particle analogous to angular momentum, along a particular dimension \cite{wiki:pauli-matrices}.
We thus have a total of $3^n$ possible experimental observables and $2^n$ possible outcomes for each observable. Based on the observable and outcome, we can create the operator $P^\mb a_{\mb s} = P^{a_1}_{s_1} \otimes \dots \otimes P^{a_n}_{s_n}$, which corresponds to the projector for this $(\mb a,\mb s)$ pair. The term $P^{a_i}_{s_i}$ corresponds to the orthogonal projection associated to the eigenvalue $s_i$ in the diagonalization of $\sigma_{a_i}$, which in turn can be rewritten as $\sigma_{a_i} = -1P^{a_i}_{-1} + 1P^{a_i}_1$ due to its Hermitian property. Note that under this notation, $\mb a$ and $\mb s$ are both vectors \ti{and} indices. Reminding ourselves of the POVM and Born's rule we saw in \ref{background:Quantum Tomography:math}, we can formulate the main equation as


\begin{equation}
    \forall \mb s \in \mathcal{R}^n, p_{\mb a,\mb s} = \Pr(R^{\mb a} = \mb s) = \tr(\rho P^{\mb a}_{\mb s})
\end{equation}
where $R^\mb{a} \in \mathcal{R}^n$ is the random vector outcome of the experiment $\mb a$ and $p_{\mb a,\mb s}$ the probability linked to that pair. We can see that we recover the projective measurement (or more generally the POVM), only extended to the multi-qubit case by the Kronecker product applied to the projectors $P^{\mb a}_\mb{ s}$.\medbreak

An important element to take into account when working with this method in practice is that, while it the theoretically correct approach, it requires on the order of $6^n = 2^n 3^n$ operations, making it costly.

% Note that there is a slight confusion between the term observable for a qubit (i.e. a Pauli matrix) and an observable to measure the system (in this case the Kronecker product of projection matrices).


\subsubsection*{Mixed qubit data generation}\label{section:mixed-qub-dg}

Mixed qubit data generation (also called ``Pauli observables" in \cite{Guta20}) is an alternative approach, where instead of calculating the probability for each observable/outcome pair, we will directly approximate the expected value linked to this observable. Its calculation is much more straightforward: if we call $A_m$ the observable of which we calculate the expected value, then $A_m = \sigma_{m_1} \otimes \sigma_{m_2} \dots \otimes \sigma_{m_n}$ with $n$ the number of qubits and $m_i$ identifying the Pauli matrix for qubit $i$ for combination $m$ (the notation is the same as in \cite{meth:bayesian:Langevin:ACMT2024}). This results in a total of $4^n$ combinations ($\sigma_{m_i} \in \{I, \sigma_x, \sigma_y, \sigma_z\}$), hence less costly than the separate qubit process.

%\section{Pointwise estimation methods}
\newpage

\section{Markov Chain Monte Carlo methods}
Markov Chain Monte Carlo (MCMC) methods are a class of sampling algorithms. Sampling from a probability distribution is relevant when we want to compute some statistic about it, for example the mean. In most cases, the distribution $\pi(\mb x)$ has high dimensionality as we work in the state of parameters of statistical models. MCMC is particularly important in the context of \ti{Bayesian inference}. In this case, $\pi(\mb x)$ corresponds to the posterior distribution from which we want to sample, and whose probability density function (PDF) is a complex expression. \medbreak

Various methods exist for sampling from simple distributions, the most famous being the inverse transform method, rejection sampling, or importance sampling. All these methods however have drawbacks: inverse transform requires an analytical solution for the cumulative density function (CDF), which is usually not possible to obtain for complex PDFs as it requires a proper integral; rejection sampling and importance sampling do not have this problem, but they suffer from the curse of dimensionality - once your dimension grows, the sampling becomes very inefficient.\medbreak

We will now cover several aspects which make up MCMC: Bayesian inference, the theoretical guarantees behind the MCMC methods and finally the 2 main algorithms we will be concerned about in this thesis, Metropolis-Hastings and Langevin sampling.

\subsection{Bayesian inference}\label{section:background:mcmc:bayesian-inference}
As its name suggests, Bayesian inference relies on Bayes theorem. In the context of algorithms, it states that given data $\mb{D}$ and parameters $\boldsymbol{\theta}$, the \textit{posterior} distribution $\pi(\theta|\mb{D})$ is calculated as 
\begin{equation} \label{eq:posterior-with-marginal}
    \pi(\boldsymbol{\theta}|\mb{D}) = \frac{\pi(\mb{D}|\boldsymbol{\theta})\pi(\boldsymbol{\theta})}{\pi(\tb D)} = \frac{\pi(\mb{D}|\boldsymbol{\theta})\pi(\boldsymbol{\theta})}{\int_{\boldsymbol{\theta}} \pi(\mb{D}|\boldsymbol{\theta}) \pi(\boldsymbol{\theta})d\boldsymbol{\theta}}
\end{equation}
where $\pi(\mb{D}|\pi(\boldsymbol{\theta}))$ is the \ti{likelihood} of data given the parameters and $\pi(\theta)$ the \ti{prior} distribution we put on the parameters. The denominator
\begin{equation}
    \pi(\mb{D}) = {\int_{\boldsymbol{\theta}} \pi(\mb{D}|\boldsymbol{\theta}) \pi(\boldsymbol{\theta})d\boldsymbol{\theta}}
\end{equation} 
is called the \ti{marginal}, and corresponds to a normalizing constant. It is in practice intractable to calculate as it involves performing a very high dimensional integral numerically, which is unstable. The good news is that in the context of MCMC methods, the normalizing factor is not important and we can safely remove it. This gives the following posterior:
\begin{equation} \label{eq:posterior-without-marginal}
    \pi(\boldsymbol{\theta}|\mb{D}) \propto {\pi(\mb{D}|\boldsymbol{\theta}) \pi(\boldsymbol{\theta})}
\end{equation}
Getting samples from $\pi(\boldsymbol{\theta}|\mb{D})$ allows us to compute integrals, a technique called Monte Carlo integration. For example, if we have a function $g$ defined on $\mathbb{R}^m$ and we want to approximate its integral on a subset $\Omega \subseteq \mathbb R^m$, we can sample $N$ values uniformly $\{x_i\}_{i=1}^N \in \Omega$ and calculate 
\begin{equation}
    \int_\Omega g(x) dx \approx \frac{1}{N} \sum_{i=1}^{N} g(x_i)
\end{equation}
The law of large numbers ensures that with $N \rightarrow \infty$, the discrete sum converges to the true integral.\cite{wiki:monte-carlo-integration}\medbreak

In statistics, integrals are of particular interest to us as they correspond to expected values. If we have function $f(\boldsymbol{\theta})$ defined on some set $\boldsymbol{\Theta}$, then
\begin{equation}
    \mathbb E_{\boldsymbol \theta \sim \pi(\btheta|\mb{D})}\left(f(\boldsymbol{\theta})\right) = \int_{\boldsymbol{\theta}} f(\boldsymbol{\theta})\pi(\btheta|\mb{D}) d\boldsymbol{\btheta} \approx \frac{1}{N} \sum_{i=1}^{n} f(\boldsymbol{\theta}_i)
\end{equation}
where $\{\boldsymbol{\theta}_i\}^N_{i=1}$ are samples from $\pi(\btheta|\mb{D})$. This allows us to compute estimates for a random variable (with $f(\btheta) = \btheta$), vector or matrix, the last one being the most relevant in Quantum Tomography. In that case, the parameters correspond to the density matrix $\rho$, and the data to the projectors $P^{\mb a}_{\mb s}$ and the empirical probabilities $\hat p_{a,s}$. By first putting a prior on $\rho$, we can obtain a sample estimate of $\rho$, balanced by the data we have. This is where the term \ti{inference} comes from, as we infer $\rho$ from the data and prior.

\subsection{Theoretical guarantees}\label{section:background:mcmc:theory}
In contrast to Monte Carlo methods where the samples are independent, MCMC builds a \ti{Markov chain} of samples $\btheta^{(1)} \dots \btheta^{(R)}$, which are dependent. As a reminder, a sequence $\mb X^{(1)}, \mb X^{(2)} \dots \mb X^{(R)}$ of random variables is a Markov chain if
\begin{equation}
    \Pr(\mb X^{(r+1)} \in A|\mb x^{(1)}, \dots, \mb x^{(r)}) = \Pr(\mb X^{(r+1)}|\mb x^{(r)})
\end{equation}
where $\mb x^{(i)}$ is a realization of $\mb X^{(i)}$ on domain $A$. One question remains: how can we make sure that samples we obtain from some Markov chain indeed come from the distribution we sample from? In order to answer this, we need to review several properties of Markov chains.\medbreak

A \ti{transition kernel} $k$ is a function that that fully characterizes a Markov chain. It gives the probability of transitioning from one state to another in chain: in the continuous case, this corresponds to conditional function $k(\mb x^{(r+1)}| \mb x^{(r)})$ and the discrete case, it is a transition matrix $P$. We say that our chain in \ti{invariant} to a distribution $\pi(\mb x)$ if 
\begin{equation}
    \pi(\mb x^*) = \int k(\mb x^*|x) \pi(\mb x)d\mb x
\end{equation}
This corresponds to saying that our chain admits a \ti{stationary} distribution, or equivalently that the \ti{marginal} distributions of $\mb X^{(r)}$ and $\mb X^{(r+1)}$ are the same. The distribution $\pi(\mb x)$, called the \ti{target} density, corresponds to the distribution we want to sample from in the context of MCMC. It is important to note that not all chains admit a stationary distribution, and a few properties must be respected. We will cover the discrete case for brevity reasons, but this of course extends to the continuous one \cite{mcmc:slides:Rigon2024Mar,wiki:markov-chain}.
\begin{itemize}
    \item \tb{Irreducibility}: a Markov chain is \ti{irreducible} if it explores the entire sample space, and does not get stuck in local regions. Formally, if $\mb X^{(r)}\in \mathbb{N}$, this corresponds to the condition 
    \begin{equation}
        \Pr(\tau_j < \infty | \mb x^{(0)} = j') > 0\quad \forall j,j' \in \mathbb{N}
    \end{equation} 
    where $\tau_j = \inf\{r\geq 1: \mb X^{(r)} = j\}$ is the first passage time for which the chain is equal to $j$ (defined as $\infty$ if $\mb X^{(r)} \neq j$ for every $r\geq 1$) and $\Pr(\tau_j < \infty | \mb x^{(0)} = j')$ the probability of return to $j$ in a finite number of steps.
    \item \tb{Aperiodicity}: a Markov chain is \ti{aperiodic} if it does not have any deterministic cycles. Formally, we say that a state $j$ is aperiodic if the set $\{r\geq1: [P^r]_{jj} > 0\}$ has no common divisor other than 1 (the matrix $P$ is the ) A chain is aperiodic if all of its states are aperiodic.
    \item \tb{Harris recurrence}:  a Markov chain is \ti{recurrent} if it visits any region of the sample space sufficiently often. Formally, a state $j$ of a discrete irreducible Markov chain is recurrent if and only if
    \begin{equation}
        \Pr(\tau_j < \infty |\mb x^{(0)} = j) = \Pr(\mb X^{(r)} = j \text{ for infinitely many } r | \mb x^{(0)}=j) = 1
    \end{equation}
    In other words, it means that this state is visited infinitely often. If we have all the states that are recurrent, then the chain is called recurrent. If additionally this chain admits an invariant PDF, then it is Harris positive (also called positive recurrent).
    \item \tb{Ergodicity}: a Markov chain in \ti{ergodic} if every state is ergodic. A state $j$ is ergodic if is aperiodic and positive recurrent. Intuitively, this property means that we can reach every state with a probability greater than 0.
\end{itemize}
We can finally state the main result that allows us to use Markov chains for MCMC \cite{mcmc:RobertCasella2004}:
\begin{boxedthm}
    A Markov chain Monte Carlo (MCMC) method converges to a distribution $\pi(\mb x)$ if the underlying Markov chain is ergodic with an equilibrium distribution $f(\mb x) = \pi (\mb x)$.
\end{boxedthm}

A very related theorem that provides a similar guarantee than the law of large number but for Markov chains is the \ti{Ergodic} theorem \cite{mcmc:slides:Rigon2024Mar}:

\begin{boxedthm}
    Let the Markov chain $(\mb X^{(r)})_{r\geq 1}$ be Harris positive with stationary distribution $\pi$, and $g$ be integrable with respect to $\pi$. Then,
    \begin{equation}
        \frac{1}{R}\sum_{r=1}^{R}g(\mb X^{(r)}) \rightarrow \int g(\mb x) \pi(\mb x) d \mb x \quad \text{when } R \rightarrow \infty
    \end{equation} 
    almost surely.
\end{boxedthm}

% \subsection*{MCMC}
% Markov Chain Monte Carlo (MCMC) methods are another class of algorithms to sample from probability distributions.
% %While we can easily sample from standard distributions using methods such as inverse transform sampling, and rejection sampling, we often have no way of sampling from more complex ones, especially those involving a high number of dimensions. 
% MCMC works by constructing a chain of random samples (which are Markovian in nature by being dependent only on the previous one) which at some point are guaranteed to converge to the stationary distribution of the density function of interest. \\\\
% In the usual setting, not all samples are accepted. The probability of acceptance of the next one $x'$ given the current $x$ depends on the ratio $\cfrac{p(x')}{p(x)}$. If the new one is more probable, it is then accepted, otherwise the probability is proportional to the ratio. The class of algorithms that involve this acceptance step are called \textit{adjusted}.\\\\
% In the usual case, MCMC is applied in the context of Bayesian inference. Given the Bayes equation 
% $$
% p(\theta|x) \propto p(x|\theta) p(\theta)
% $$
% we want to obtain samples from the posterior distribution $p(\theta|x)$. This corresponds to getting a distribution for the parameters $\theta$ of our model, given a prior that we have placed over them (our a priori knowledge of how they are distributed) and a likelihood that balances out the prior with data $x$ we have. The Bayes equation in its full form involves a term called the marginal $p(x)$ in the denominator, however as it does depend on the parameters, we do not have to include it.

% %\section{MCMC for Quantum Tomography}
% The samples obtained after MCMC methods are usually used to approximate the distribution of the parameters. In the case of matrix estimation, there are many ways to use these samples. One example is to proceed as in \cite{MA17}, where they construct rank 1 matrices with each sample and then weight them in such a way that only one of the matrices contributes to $\rho$:
% $$
% \hat \rho = \sum_{i=1}^d \gamma_i V_i V_i^{\dagger}
% $$
% The $\gamma_i$ are distributed according to a $\mb{D}ir(\alpha, \dots, \alpha)$ which promotes sparsity among them.


% More formally, given a target distribution $f(x|\theta)$ we can evaluate and proposal distribution  TODO
\subsection{Metropolis-Hastings algorithm}\label{section:Metropolis-Hastings}
The \ti{Metropolis-Hastings} algorithm is the most well-known and used MCMC algorithm. The Markov chain of samples is built by doing a random walk, with an acceptance/rejection step. It works as follows: given that we are currently located at $\mb x \in \mathbb{R}^n$, we will sample a potential candidate $\mb x^*$ from $q(\mb x^*|\mb x)$ where $q$ is called the \ti{proposal} (or \ti{jumping}) distribution. This $q$ is analogous to the kernel we saw in \ref{section:background:mcmc:theory}, although not exactly as we will come back to it later. Next, we will compute the \ti{acceptance} probability of this sample:

\begin{equation}
    \alpha(\mb x^*, \mb x) = \min\left(1, \frac{\pi(\mb x^*) q(\mb x|\mb x^*)}{\pi(\mb x) q(\mb x^*|\mb x)}\right)
\end{equation}
where $\pi(\mb x)$ is the posterior distribution we would like to sample from. Note that we can simplify this criterion in case the proposal density is symmetric ($q(\mb x^*|\mb x) = q(\mb x|\mb x^*)$):
\begin{equation}
    \alpha(\mb x^*, \mb x) = \min\left(1, \frac{\pi(\mb x^*)}{\pi(\mb x)}\right)
\end{equation}
This happens often in practice, as the Gaussian distribution is a common choice of proposal.
A second comment is that, as mentioned in section \ref{section:background:mcmc:bayesian-inference}, the normalizing constant of the posterior does not matter: it gets simplified in the numerator and denominator as it does not depend on $\mb x$, nor $\mb x^*$ (only the data \tb D, see equation \ref{eq:posterior-with-marginal}). \medbreak

After this computation, we will accept $\mb x^*$ with probability $\alpha$. In practice, this involves for example sampling a value $u \sim U(0,1)$, and accepting if $ u \leq \alpha$. This procedure is repeated until the desired number of samples is obtained, with some part of samples that we discard at the beginning. This period is called the \ti{burn-in} period and corresponds to the duration when the samples we obtain are not part of the equilibrium distribution.
We usually talk about \textit{convergence} when the samples we get seem to come from $\pi$, or that the Markov chain has \textit{mixed} (based on a visual assessment for example). A pseudocode for the algorithm could be:\medbreak

\begin{algorithm}[H]
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    %\underline{Prob-estimator}\;
    \Input{$\mb x^{(0)} \in \mathbb{R}^n, T \in \mathbb{N}, B \in \mathbb{N}$}
    \Output{$\mb x^{({B+1})} \in \mathbb{R}^n, \mb x^{(B+2)}, \dots, \mb x^{(T)}$}
    \For{$t\gets 1:T$}{ 
        Sample $\mb x^* \sim q(\mb x^*|\mb x^{(t-1)})$\;
        $\alpha = \min\left(1, \frac{\pi(\mb x^*) q(\mb x^{(t-1)}|\mb x^*)}{\pi(\mb x^{(t-1)}) q(\mb x^*|\mb x^{(t-1)})}\right)$\;
        Sample $\mb x^* \sim U(-0.5, 0.5)$\;
        \eIf{$u \leq \alpha$}{
            \tcp*[l]{Accept $\mb x^*$}
            $\mb x^{(t)} \gets \mb x^*$
        }{
            \tcp*[l]{Reject $\mb x^*$}
            $\mb x^{(t)} \gets \mb x^{(t-1)}$
        }
    }
    \caption{Metropolis-Hastings algorithm}
\end{algorithm}\medbreak

Note that in most implementations, it is common practice to use $\log(\alpha)$ for the calculation of the acceptance probability, instead of $\alpha$ directly. This allows to avoid numerical problems due to divisions by small numbers and underflow/overflow that might occur when handling double precision floats. In addition, using the logarithm combines nicely with distributions based on the exponential family, allowing for a $\log(\exp)$ simplification.\medbreak

We have in section \ref{section:background:mcmc:theory} talked about convergence for Markov chains to the posterior distribution, but have not talked how that would work in practice. A very handy way is to use the \ti{detailed balance} condition.

\begin{boxedthm}
    A Markov chain $(\mb X^{(r)})_{r\geq1}$ with transition kernel $k$ satisfies the detailed balance if there exists a function $f$ such that 
    \begin{equation}
        k(\mb x|\mb x^*) f(\mb x) = k(\mb x^*|\mb x) f(\mb x^*)
    \end{equation}
\end{boxedthm}
In this context, we also talk about \ti{reversibility}:

\begin{boxedthm}
    a Markov chain $(\mb X^{(r)})_{r\geq1}$ is \ti{reversible} if
\begin{equation}
    \Pr(\mb X^{(r)} | \mb X^{(r+1)}) = \Pr(\mb X^{(r+1)} | \mb X^{(r)})
\end{equation}
\end{boxedthm}
Finally, we can reach an important and useful result for MCMC:

\begin{boxedthm}
    if a Markov chain $(\mb X^{(r)})_{r\geq1}$ satisfies the detailed balance with $\pi$ a probability density function, then $\pi$ is the stationary density, and chain is reversible.
\end{boxedthm}

This result means that, as long as we satisfy the detailed balance with our MCMC algorithm, we will converge to the posterior distribution $\pi$. In the case of Metropolis-Hastings, the transition kernel is 
\begin{equation}
    k\left(\boldsymbol{x}^* \mid \boldsymbol{x}\right)=\alpha\left(\boldsymbol{x}^*, \boldsymbol{x}\right) q\left(\boldsymbol{x}^* \mid \boldsymbol{x}\right)+\delta_{\boldsymbol{x}}\left(\boldsymbol{x}^*\right) \int q(\boldsymbol{s} \mid \boldsymbol{x})\{1-\alpha(\boldsymbol{s} \mid \boldsymbol{x})\} \mathrm{d} \boldsymbol{s}
\end{equation}
where $\delta_{\mb x}(\mb x^*)$ is a point mass at $\mb x$. Putting this kernel into the detailed balance condition, one can derive than the equality indeed holds. TODO: derive in appendix? The only element that we have not discussed so far is the ergodicity condition. Altough is depends on $\pi$ and $q$, it is typically true under very mild conditions, which can be checked in, for example, \cite{mcmc:RobertCasella2004}.\medbreak

A final element to take into account with theoretical convergence guarantees is that in real-world experiments, the algorithm may take a very long time converge, making it unpractical for real use. In these situations, one could for example investigate different proposals, change the duration of the burn-in period, or even swap Metropolis-Hastings for another algorithm. Various metrics exist to quantify the convergence, such as the effective sample size or $\hat R$, but also visual checks, which are effective for checking if the chain has mixed. It is also common practice to run several chains from different starting points, and assess the overall performance. More details can be found in \cite{mcmc:gelman2013bayesian}.

\subsection{Gibbs algorithm}\label{section:gibbs}
The Gibbs algorithm takes a different approach compared to Metropolis-Hastings in that it splits the joint distribution $\pi(\mb x) = \pi(x_1, x_2, \dots, x_n)$ with $\mb x \in \mathbb{R}^n$ into one-dimensional conditionals, and then step samples from each conditional $\pi(x_i|\mb x_{-i}) = \pi(x_i|x_1,\dots,  x_{i-1}, x_{i+1}, \dots, x_n)$ iteratively. It can also be generalized to blocks, where instead of sampling from a one-dimensional distribution, we sample from a joint conditional. \medbreak

It is, in its simplest form, an \ti{unadjusted} algorithm, as it does not involve an accept/reject step.
A pseudocode for it can be the following:\medbreak

\begin{algorithm}[H]
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{$\mb x^{(0)}, T, B$}
    \Output{$\mb x^{(B+1)}, \mb x^{(B+2)}, \dots, \mb x^{(T)}$}
    \For{$t\gets 1:T$}{ 
        $\mb x^{(t)} \gets \mb x^{(t-1)}$\;
        \For{$i\gets 1:n$}{
            \tcp*[l]{Sample from the conditional and set $i$-th element to this value}
            $\mb x_{i}^{(t)} \sim \pi(x_i|\mb x_{-i}^{(t)})$\;
        }
    }
    \caption{Gibbs algorithm}
\end{algorithm}
Note that the inner loop updates the current $\mb x^{(t)}$, and not a copy. This implies that at iteration $i$, we are using the conditioned value set at $i-1$ for $x^{(t)}_{i-1}$, and not the one set at $x^{(t-1)}_{i-1}$.\medbreak

The main advantages of Gibbs sampling is that we do not need to tune the proposal distributions, as well as the fact that all samples are accepted. The downside of course is that we need to be able to derive and sample from the conditional, especially doing so efficiently as otherwise this approach becomes very costly. A more general algorithm, called \textit{Metropolis-within-Gibbs}, involves sampling from the conditional using Metropolis-Hastings, and in that case this may involve an accept/reject step. In the case of standard Gibbs, we may view the conditionals we sample from as proposal distributions $q(\mb x^*|\mb x)$. This in turn shows us why this algorithm is unadjusted, as the acceptance probability $\alpha$ is always equal to 1 \cite{mcmc:slides:Rigon2024Mar}. If we call $\mb x^* = (x_1^{(t)}, \dots, x_{i}^*, \dots, x^{(t)}_{n})$ and $\mb x = (x_1^{(t)}, \dots, x_{i}^{(t)}, \dots, x^{(t)}_{n})$, then

\begin{equation}
    \alpha(\mb x^*|\mb x) = \frac{\pi(\mb x^*) q(\mb x|\mb x^*)}{\pi(\mb x) q(\mb x^* |\mb x)} = \frac{\pi(x_i^*|\mb x_{-i}^*)\pi(\mb x^*_{-i}) \pi(x_i|\mb x^*_{-i})}{\pi(x_i|\mb x_{-i})\pi(\mb x_{-i}) \pi(x_i^*|\mb x_{-i})} = \frac{\pi( x_i^*|\mb x_{-i})\pi(\mb x_{-i}) \pi(x_i|\mb x_{-i})}{\pi(x_i|\mb x_{-i})\pi(\mb x_{-i}) \pi(x_i^*|\mb x_{-i})} = 1
\end{equation}
where we have used the fact that $\mb x^*_{-i} = \mb x_{-i}$.


\subsection{Unadjusted Langevin algorithm}\label{section:ula}
One big down-side with Metropolis-Hastings or Gibbs sampling is that they can take a long time to converge, especially in a very high-dimensional setting. The Unadjusted Langevin algorithm (ULA) tackles this problem by using the gradient information about the posterior. The downside, of course, is that we must be able to calculate it, but in the case we can, it can drastically speed-up the convergence. The algorithm draws its name from \ti{Langevin diffusion} (also called \ti{Îto diffusion}), a stochastic differential equation (SDE), written as follows:
\begin{equation}
    \mathrm{d}X_t = -\nabla V(x)\mathrm{d}t + \sqrt{2}\mathrm{d}B_t
\end{equation}
where $X_t$ is the position of a particle in a potential $V(x)$ and $B_t$ is the time derivative of the standard Brownian motion. A SDE is a differential equation where one or more of the terms is a stochastic process, and results in a solution which is also a stochastic process. In the case of Langevin diffusion, the solution happens to be a stationary process $p_{\infty}(x) \propto \exp(-V(x))$, which is very handy in the context of MCMC. If we set $V(x) = -\log(\pi(x))$ (we assume a one-dimensional density here), then we can simulate the SDE and obtain samples from $p_\infty = \pi$:
\begin{equation}    
    \mathrm{d}X_t = \nabla \log(\pi(x))\mathrm{d}t + \sqrt{2}\mathrm{d}B_t
\end{equation}

Of course, to simulate, we must discretize the SDE in some way, and in this case \textit{Euler-Maruyama approximation} is what is commonly used. This results in
\begin{equation}
    X^{(k+1)} - X^{(k)} = \tau \nabla \log\pi(X^{(k)}) + \sqrt{2\tau}(B^{(k+1)} - B^{(k)})
\end{equation}
where $\tau$ is a step size that can be changed over time and $(B^{(k+1)} - B^{(k)}) \sim N(\mb 0, \mb I)$. We can then rewrite it as follows:
\begin{equation}
    X^{(k+1)} = X^{(k)} + \tau \nabla \log\pi(X^{(k)}) + \sqrt{2\tau}\xi^{(k)}
\end{equation}
where $\xi_k \sim N(\mb 0, \mb I)$. Note that the true solution can be computed with $X(k\tau)$. The pseudocode can be written as follows (we switch back to using $\mb x \in \mathbb{R}^n$ here):\medbreak
\begin{algorithm}[H]
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{$\mb x^{(0)}, K, B, \tau_{1:K}$}
    \Output{$\mb x^{(B+1)}, \mb x^{(B+2)}, \dots, \mb x^{(K)}$}
    \For{$k\gets 1:K$}{ 
        $\boldsymbol{\xi}^{(k)} \sim N(\mb 0, \mb I)$\;
        $\mb x^{(k+1)} = \mb x^{(k)} + \tau_k \nabla \log\pi(\mb x^{(k)}) + \sqrt{2\tau_k}\boldsymbol{\xi}^{(k)}$\;
    }
    \caption{Unadjusted Langevin algorithm}
\end{algorithm}\medbreak

ULA can be seen as noisy gradient ascent, which is why it performs better than Metropolis-Hastings or Gibbs. By utilizing the gradient information, we are able to sample more often the regions of high density. The cost of computing the gradient is usually compensated by faster convergence times, making it often an algorithm of choice if the gradient of the posterior can be computed. Note that there also exists a method called \textit{Metropolis-adjusted Langevin algorithm} (MALA), which generalizes this approach with an accept/reject step, but we will not go into the details here \cite{mcmc:langenvin-blog:Ansari2024Apr, wiki:MALA}.

TODO: talk about theoretical guarantees?

\chapter{Overview of relevant methods} \label{section:relevant-methods}
We will introduce here 2 MCMC algorithms that are in relevant in the context of this thesis: the \ti{prob-estimator} and the \ti{Projected Langevin algorithm}. They will be useful in Chapter \ref{section:numerical-exp} where we will be comparing them in numerical experiments and Chapter \ref{section:algo-vs-prior} where we will be presenting 2 new algorithms derived from them.

\section{Prob-estimator}
Introduced in \cite{MA17}, the \textit{prob-estimator} is a Metropolis-Hastings-based algorithm (more details about it can be found in section \ref{section:Metropolis-Hastings}), where the approximation of the density matrix $\rho$ is done using a sum of rank 1 matrices and sparse coefficients, leading to a rank 1 approximation. The exact solution is written as follows:
\begin{equation}\label{eq:rank1-approx}
    \rho = \sum_{i=1}^d \gamma_i V_i V_i^\dagger
\end{equation}
where $V_i \in \mathbb{C}^{d\times 1}$ are normalized vectors and $\gamma_i \in \mathbb{C}$ are the sparse coefficients with $\sum_{i=1}^d \gamma_i = 1$.\medbreak

We can check that the properties of a density matrix are respected: it is a sum of Hermitian matrices, which also makes $\rho$ Hermitian and positive semi-definite and 
\begin{equation}
    \tr(\rho) = \sum_{i=1}^{d} \gamma_i \tr(V_i V_i^\dagger) = 1
\end{equation}
as $\tr(V_i V_i^\dagger) = 1$ due to the normalization.\medbreak
% \begin{equation}
% \hat \rho = \frac{1}{T}\sum_{t=1}^{(t)}  \left(
% \sum_{i=1}^d \gamma_i^{(t)} V_i^{(t)} (V_i^{{(t)}})^\dagger 
% \right)
% \end{equation}

This way of calculating $\rho$ is analogous to taking an eigenvector decomposition
\begin{equation}
    \rho = U\Lambda U^\dagger
\end{equation}
where $U = (U_1|\dots|U_d)$ with $U_i$ orthogonal to each other and of norm 1, and $\Lambda = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_d)$ with $\sum_{i=1}^d \lambda_i = 1$ (due to $\rho$ being a density matrix).\medbreak

The advantage of the approach from equation \ref{eq:rank1-approx} is that it does not require the $V_i$ to be orthonormal (as they are in the case of a decomposition of a symmetric/Hermitian matrix). In some sense, the parameter $\gamma_i$ can be thought of as an eigenvalue and $V_i$ its associated eigenvector, even though this is not exactly true as the collection $\{V_i\}$ is not orthogonal. \medbreak

As we are using an MCMC method, we need to formulate the posterior $\pi(\nu | \tb D)$ from which we will be sampling, with $\nu = \sum_{i=1}^d \gamma_i V_i V_i^\dagger$ our parameter matrix and the data $\mb D$ comprised of the projectors $P^{\mb a}_{\mb s}$ and empirical probabilities $\hat p_{\mb a,\mb s}$. In the case of the prob-estimator, the authors use a pseudo-likelihood written as follows
\begin{equation}
    \pi(\tb D| \nu) = \mathcal{L}(\nu, \mb D) = \exp(-\lambda \ell(\nu,\mb D))
\end{equation}
where $\lambda$ is a scaling parameter and $\ell(\nu, D)$ the loss function
\begin{equation}
    \ell(\nu, \mb D) = \sum_{\mb a \in \mathcal{E}^n} \sum_{\mb s \in \mathcal{R}^n} \left[\tr(\nu P^{\mb a}_{\mb s}) - \hat p_{\mb a,\mb s}\right]^2
\end{equation}\medbreak
They use separate qubit data generation, thus all details regarding notation can be checked in \ref{section:sep-qub-dg}.
The prior $\pi(\nu) = \pi_1(V_1, \dots, V_d) \pi_2(\gamma_1, \dots, \gamma_d)$ is defined in 2 parts:
\begin{itemize}
    \item For $\pi_1(V_1, \dots, V_d)$, the set $V_1, \dots, V_d$ is i.i.d uniformly distributed on the unit sphere. In practice, this involves first sampling from a standard normal multivariate $\tilde V_i \sim N(\mb 0, \mb I)$, and then normalizing it $V_i = \tilde V_i/||\tilde V_i||$.
    \item For $\pi_2(\gamma_1, \dots, \gamma_d)$, the set $\gamma_1,\dots,\gamma_d$ is sampled from a Dirichlet distribution $\mathcal{D}ir(\alpha_1,\dots,\alpha_d)$ with parameter $\alpha_i > 0$. In order to promote sparsity among $\gamma_1, \dots, \gamma_d$ and get a rank-1 approximation of $\rho$, the parameters are all chosen to be the same, small value: $\alpha_1,\dots,\alpha_d = \alpha = 1/d$. This results in a typical drawing of one $\gamma_i$ close to 1, while others close to 0. In practice, the authors do not sample the Dirichlet distribution directly, and instead use an equivalent formulation: $\gamma_i = Y_i/\sum_{j=1}^{d} Y_j$ where $Y_i$ is sampled from a Gamma distribution $Y_i \sim Gamma(\alpha, 1)$.   
\end{itemize}
The posterior is then written as 
\begin{equation}
    \pi(\nu|\tb D) \propto \exp(-\lambda\ell(\nu, \mb D)) \pi(\nu)
\end{equation}
and estimated with 
\begin{equation}
    \hat \rho_{\text{prob}} = \int \nu \pi(\nu|\tb D) d\nu
\end{equation}

As specified above, the prior is split in 2 parts. This is due to way the algorithm is structured: inside the main loop of Metropolis-Hastings (which goes over iterations $t$), we also iterate over each dimension $i$, first for $\gamma$, and then for $V$. This can be seen as doing Gibbs-within-Metropolis-Hastings. \medbreak

The pseudocode for the algorithm is available below:\medbreak

\begin{algorithm}[H]\label{code:prob}
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    %\underline{Prob-estimator}\;
    \Input{$d = 2^n, \lambda \in \mathbb{R}, \alpha \in \mathbb{R}, T \in \mathbb{N}, V^{(0)} \in \mathbb{C}^{d \times d}, Y^{(0)} \in \mathbb{C}^{d\times 1}$}
    \Output{$\hat \rho \in \mathbb{C}^{d\times d}$}
    $\hat \rho \gets \mb 0$\;
    $\gamma^{(0)} \in \mathbb{R}^{d\times1} \gets Y^{(0)} /\sum_{i=1}^d Y_i^{(0)}$\;
    \For{$t\gets 1:T$}{ 
    $Y^{(t)} \gets Y^{(t-1)}$\;
    \tcp*[l]{Update $\gamma$}
            \For{$i\gets 1:d$}{
            $\tilde Y \gets Y^{(t)}$\;
            Sample $y \sim U(-0.5, 0.5)$\;
            $\tilde Y_i \gets Y_i^{(t)} \exp (y)$ \;
            $\tilde \gamma \gets \tilde Y /(\sum_{j=1}^d \tilde Y_j)$\;
            $Y_i^{(t)} \gets \begin{cases}
                \tilde Y_i \hspace{0.96cm} \text{with probability} \min\{R(\tilde Y, Y^{(t)}, \tilde \gamma, \gamma^{(t)}, V^{(t-1)}), 1\} \tcp*[l]{$R$ defined below}\\
                Y_i^{(t)} \hspace{0.63cm}\text{otherwise}
            \end{cases}$
            $\gamma^{(t)} \gets Y^{(t)} /\sum_{k=1}^d Y_k^{(t)}$ \tcp*[l]{Update all components}
            }
            \;
            $V^{(t)} \gets V^{(t-1)}$\;
            \tcp*[l]{Update $V$}
            
            \For{$i\gets 1:d$}{
            $\tilde V \gets V^{(t)}$\;
            Sample $V \sim N(\mathbf{0}, \mathbf{I})$\;
      
            $\tilde V_i \gets (V + V_i^{(t)})/||V + V_i^{(t)}||_2$\tcp*[l]{Sample from the unit sphere}
            
            $V_i^{(t)} \gets \begin{cases}
                \tilde V_i \hspace{0.96cm} \text{with probability} \min\{A(\tilde V, V^{(t)}, \gamma^{(t)}), 1\} \tcp*[l]{$A$ defined below}\\
                V_i^{(t)} \hspace{0.63cm}\text{otherwise}
            \end{cases}$
        }
        \;
        \tcp*[l]{Compute a running average estimate}
        $\hat \rho \gets \frac{1}{t}\sum_{i=1}^{d} \gamma_i^{(t)} V_i^{(t)} (V_i^{(t)})^\dagger  + (1 - \frac{1}{t})\hat \rho $ 
    }
    \caption{Prob-estimator algorithm}
\end{algorithm}\medbreak

Here, we have that 
\begin{align}
\log R(\tilde Y, Y^{(t)}, \tilde \gamma, \gamma^{(t)}, V^{(t-1)}) &= - \lambda \ell^{}(\sum_{i=1}^{d} \tilde \gamma_i V_i^{(t-1)} (V_i^{(t-1)})^\dagger, \mb D) + \lambda \ell^{}(\sum_{i=1}^{d} \gamma_i^{(t)} V_i^{(t-1)} (V_i^{(t-1)})^\dagger, \mb D) \\
&\quad + (\alpha - 1) \log\left(\frac{\tilde Y_i}{Y_i^{(t)}}\right) - \tilde Y_i + Y_i^{(t)}
\end{align}

and 
\begin{equation}
\log A(\tilde V, V^{(t)}, \gamma^{(t)}) = -\lambda \ell(\sum_{i=1}^{d} \gamma_i^{(t)} \tilde V_i \tilde V_i^\dagger, \mb D) + \lambda \ell^{}(\sum_{i=1}^{d} \gamma_i^{(t)} V_i^{(t)} (V_i^{(t)})^\dagger, \mb D) 
\end{equation}
The curious reader can find the detailed derivations in the appendix \ref{section:appendix:acc-rate-prob}.\medbreak


 Note that there is an element which is ambiguous in the prob-estimator. The lines 7 and 8 of pseudocode \ref{code:prob} correspond to the sampling from $Gamma(\alpha, 1)$. It is however unclear on why this would correspond to obtaining $\tilde Y_i \sim Gamma(\alpha, 1)$. There exists a classical procedure where one uses the inverse transform method and the fact that if $X_i \sim \exp(\lambda)$, then 
\begin{equation}
    \sum_{i=1}^{t} X_i \sim Gamma(t,\lambda)
\end{equation}
however this only works in the case where $t \in \mathbb{N}$. The wikipedia page \cite{wiki:gamma-dist} mentions that the case with $0 < \alpha < 1$ (assumed with $\beta=1$) is the most challenging to sample from, with new research \cite{mcmc:gamma-sampling:liu2015simulating} still being done on this topic. The Python library \texttt{numpy} does it using a form of rejection sampling\footnote{\texttt{\url{https://github.com/numpy/numpy/blob/main/numpy/random/src/distributions/distributions.c\#220}}}, similar to \cite{mcmc:gamma-sampling:liu2015simulating}. These approaches are however very different from what is done in the prob-estimator, and as the authors do not indicate why it is correct, it is difficult to understand the reasoning behind it.

\newpage
\section{Projected Langevin}

Introduced in the \cite{meth:bayesian:Langevin:ACMT2024} paper (unpublished at the moment of submission for this thesis), the projected Langevin approach is based on an Unadjusted Langevin algorithm (ULA) (more details about it can be found in section \ref{section:ula}). The authors utilize the Burer-Monteiro factorization (introduced in \cite{proj-langevin:Burer2003}) $\rho = Y Y^\dagger$ with $Y \in \mathbb{C}^{d \times r}$ (with $d=2^n$ and $n$ the number of qubits), which corresponds to a low rank (of rank $r$) factorization of $\rho$. This corresponds to consider that the density matrix has a maximum rank of $r$. This also allows us to reduce the dimension of the parameter space from $2d^2$ to $2dr$ by running the MCMC method directly on the $Y$ space. It is important to note that the physical constraints of the problem are still respected, as we assume the matrix $Y$ to belong to the complex hypersphere $C \mathbb{S}^{d\times r} = \{ Y \in \mathbb{C}^{d\times r}: ||Y||_F = 1\}$, leading to a unit trace. The Hermitian and positive semi-definite properties of the density matrix are obtained by construction.\medbreak

In addition to the factorization, the paper proposes a new prior, called the spectral scaled Student distribution defined as
\begin{equation}    
\nu_{\theta} (Y) = C_\theta \det(\theta^2I_d + YY^\dagger)^{-(2d+r+2)/2}
\end{equation}
where $\theta$ is a scaling parameter.\medbreak

This prior is a complex extension of a prior proposed by \cite{Dal20}, where the student-t distribution is a generalization of the Gaussian distribution, allowing for fatter tails. This feature is what promotes sparsity in the eigenvalues of $Y$, favoring a low rank (most samples are close to 0, but occasionally some are large).\medbreak
The used likelihood is similar to the prob-estimator, and is written as 
\begin{equation}
L(Y, \mb D) = \sum^{M}_{i=1} (\hat p_m - \tr(A_mYY^\dagger))^2
\end{equation}
with $A_m$ the Hermitian matrix characterizing the $m^{th}$ experiment and $\hat p_m$ a measured probability, corresponding to the data $\mb D$. The authors use mixed qubit data generation, for which more details can be found in section \ref{section:mixed-qub-dg}. \medbreak
Combining these terms, we can obtain a posterior $\hat \nu_{\lambda, \theta}(Y, \mb D) = \exp(-f_{\lambda, \theta}(Y, \mb D))$ with
\begin{equation}
    f_{\lambda, \theta}(Y, \mb D) = \lambda \sum^{M}_{i=1} (\hat p_m - \tr(A_mYY^\dagger))^2 + \cfrac{2d + r + 2}{2}\log \det(\theta^2I_d + YY^\dagger)
\end{equation}
The exponential in this case is useful to us because it allows us to simplify the calculations. ULA requires us to set the potential $V(x) = -\log(\pi(x))$ where $\pi(x)$ is the posterior from which we want to sample, and using this formulation we obtain a $\log(\exp)$ simplification.\medbreak
The authors also avoid dealing with complex values by using an isomorphism $\psi: \mathbb{C}^{d\times r}\rightarrow \mathbb{R}^{2d\times 2r}$
\begin{equation}
\psi: M^R + iM^I \rightarrow \frac{1}{\sqrt 2} \begin{pmatrix}
    M^R & - M^I\\
    M^I & M^R
\end{pmatrix}
\end{equation}

This results in the following rewrite of $f$:
\begin{equation}    
\tilde f_{\lambda, \theta}(Y, \mb D) = \lambda \sum^{M}_{i=1} (\hat p_m - \sqrt{2}\tr(\tilde A_m \tilde Y \tilde Y^{T}))^2 + \cfrac{2d + r + 2}{4}\log \det(\cfrac{\theta^2}{\sqrt 2}I_{2d} + \sqrt 2 \tilde Y \tilde Y^{T}) + \tilde C
\end{equation}

where $\tilde Y = \psi(Y)$, $\psi(YY^\dagger) = \sqrt{2} \psi(Y)\psi(Y)^{T}$ and $\tilde C$ a normalization constant. We can then also obtain the gradient of $\tilde f_{\lambda, \theta}$:
\begin{equation}
\nabla \tilde f_{\lambda, \theta}(\tilde Y, \mb D) = -2\sqrt 2 \lambda \sum^{M}_{i=1} (\hat p_m - \sqrt{2}\tr(\tilde A_m \tilde Y \tilde Y^{T}))^2 (\tilde A_m + \tilde A^{(t)}_m)\tilde Y + \cfrac{2d + r + 2}{\theta^2} \left(I_{2d} + \cfrac{2}{\theta^2}\tilde Y \tilde Y^{T}\right)^{-1} \tilde Y
\end{equation}

They also use a trick (Sherman-Morrison-Woodbury formula) to compute the inverse more efficiently:
\begin{equation}
\left( I_{2d} + \frac{2}{\theta^2} \tilde Y \tilde Y^{T} \right)^{-1} =  I_{2d} - \tilde Y \left( \frac{\theta^2}{2} I_{2r}+\tilde Y^{T} \tilde Y \right)^{-1} \tilde Y^{T}
\end{equation}
Putting all these components together, we can write the pseudocode for the resulting algorithm:

\begin{algorithm}[H]
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    %\underline{Prob-estimator}\;
    \Input{$T \in \mathbb{N}, B \in \mathbb{N}, Y_0 \in \mathbb{C}^{d \times r}, \{\eta_k | k\in 1\dots T\}, \beta \in \mathbb{R}, \theta \in \mathbb{R}, \lambda \in \mathbb{R}$}
    \Output{$\tilde Y \in \mathbb{R}^{d\times r}$}
    $\tilde Y_0 \gets \psi(Y_0)$\;
    \For{$k\gets 1:T$}{ 
        $w_k^R, w_k^I \sim N(0, 1)^{d\times r}$\tcp*[l]{Sample from the standard normal of size ${d\times r}$}
        $w_k \gets w_k^R + i w_k^I$\;
        $\tilde w_k \gets \psi(w_k)$\;
        $\tilde Y_k \gets \tilde Y_{k-1} - \eta_k \nabla f  (\tilde Y_{k-1}, \theta, \lambda) + \cfrac{\sqrt{2\eta_k}}{\beta} \tilde w_k$\;
    }
    $\tilde Y \gets \cfrac{1}{T - B} \sum_{k=B+1}^{T} \tilde Y_{k}$\;
    \caption{Projected Langevin}
\end{algorithm}\medbreak
where $\eta_k$ is the step size, $\theta$ a scaling parameter for the prior, $\beta$ a scaling parameter for the noise term, $B$ the number of burn-in iterations and $T$ the number of iterations. The main differences from the original algorithm described in \ref{section:ula} are that instead of doing ascent, we are doing descent, and that we sample $ d\times r$ scalar entries from a standard normal distribution, instead of sampling $r$ $d$-dimensional vectors from a multivariate normal.\medbreak

The main motivation behind this approach comes from the faster convergence times, especially in high-dimensional settings where Metropolis-Hastings suffers. The proposed prior allows for a computable gradient, which bootstraps the use of Langevin sampling. A second benefit comes from the Burer-Monteiro factorization, which allows to use, if available, the information about the rank of density matrix. In general, the parameter $r$ in this factorization corresponds to an upper-bound on the rank of $\hat \rho$, the approximation of the true density matrix $\rho$. This however also means that we may give too much freedom to the approximation if we know that the true $\rho$ is of low rank. Setting $r$ to the true rank allows us to constrain the search space.\medbreak

TODO: talk about the Crank-Nicholson preconditioning  



\chapter{Numerical experiments}\label{section:numerical-exp}

We will now focus on the main contribution of this thesis, namely numerical experiments to compare the prob-estimator (which we will refer to as prob in figures) and the Projected Langevin algorithm (which we will refer to as PL in figures).\medbreak

The goal with these experiments is to see how does Metropolis-Hastings compare to Langevin sampling with different experimental settings in the context of Quantum Tomography. In particular, we will use the $L_2$ squared error as the main metric to assess performance in 5 main contexts: baseline convergence, influence of the burn-in period, impact of the number of shots, impact of the the number of measurements and finally impact in case we have knowledge about the rank of the matrix (in particular for Langevin). The term ``shots'' here refers to the number of times the quantum system is replicated and measured. The term ``measurements'' (albeit somewhat ambiguous) refers to the number of observables we use. For that experiment in particular, this implies that the number of observables will sometimes be less than the maximum, hence a case of tomographic incompleteness. \medbreak

As mentioned in the introduction \ref{section:goals-contributions}, the code written for these experiments will be made available later on TODO: fix incorrect link \href{www.github.com/daqwes/thesis}{GitHub}. All of it is in Python, with the prob-estimator reimplemented from R and the Projected Langevin algorithm from Matlab, with original source code provided by the authors in both cases (\cite{MA17} and \cite{meth:bayesian:Langevin:ACMT2024} respectively). The prob-estimator originally uses separate qubit data generation and Projected Langevin mixed qubit, however we will sometimes only keep one of them (this will be indicated) to use for both. Per our tests, the results are usually very close between them (for example by using separate qubit for Projected Langevin or vice versa), so either data generation approach should be valid for any experiment, regardless of the used algorithm.\medbreak

Before going in more detail about each experiment, it is important to note that there are some discrepancies for the prob-estimator between the paper and the associated implementation in R:
\begin{itemize}
\item The original paper states the proposed $\tilde V_i$ does not depend on the previous sample ($\tilde V_i$ is sampled uniformly from the uniform distribution). This however contradicts their implementation, which considers $\tilde V_i = V_i^{(t)} + V$, where $V$ is sampled from the unit sphere. Both approaches seem valid: in the former case, while we do not sample locally, we still explore the parameter space as it constrained to be the unit sphere; in the latter case, we also remain on the unit sphere due to normalization. In our implementation, we decided to align on their code.
\item Their paper does not mention the use of a running average, but this is technique they apply in the code. In this case, we also align on their code. TODO: add/remove this? and reflect that for Projected Langevin, to have a fair comparison.
\item In their code, the first sample they provide to the MCMC algorithm is not random, and comes from an estimation using the inverse method. This provides a warm start and can affect the accuracy of the algorithm. In our experiments, we use a the same random first sample for both algorithms. 
\end{itemize}

\section{Baseline comparison}\ref{section:baseline-comparison}

We will start with a baseline comparison with a convergence plot. We will $n$ to denote the number of qubits,  is the n

\section{Impact of burnin duration}

\section{Impact of number of shots}

\section{Impact of number of measurements}

\section{Impact of knowledge of rank}

\chapter{Algorithm vs prior: what matters the most?}\label{section:algo-vs-prior}

\section{Introducing new algorithms: MHS and MHGS}

\section{Proposal choice}

\section{Numerical comparison}



\chapter{Conclusion}

\chapter{Future work}
\begin{enumerate}
    \item Do more tests, in particular, do tests with shots/measurements/ranks including mhs/mhgs
    \item Include tests related to time, as langevin converges much faster than prob
    \item Test more low-rank priors, not limited to student-t
    \item Try to combine the prior in prob with langevin
    \item Test more MCMC algos, such as HMC
\end{enumerate}

% \bibliographystyle{unsrt}
% \bibliography{bibliography}
\printbibliography

\chapter*{Appendix}
\section*{Derivation of the acceptance rate for the prob-estimator}\label{section:appendix:acc-rate-prob}

\subsection*{Derviation of $\log(R)$}
TODO: clean-up this part 
The first expression $\log(R(\tilde Y, Y^{(t)}, \tilde \gamma, \gamma^{(t)}, V^{(t-1)}))$ (which we will write as $\log(R(\nu^*, \nu))$) is derived as follows: first, let us define
\begin{align}
    \tilde \ell &= \ell^{}(\sum_{i=1}^{d} \tilde \gamma_i V_i^{(t-1)} (V_i^{(t-1)})^\dagger, \mb D)\\
    \ell^{(t)} &= \ell^{}(\sum_{i=1}^{d} \gamma_i^{(t)} V_i^{(t-1)} (V_i^{(t-1)})^\dagger, \mb D)
\end{align}
In this case of the prob-estimator, we do not have a proposal distribution per se (the sampling from $Gamma(\alpha, 1)$ corresponds to the prior), so the acceptance probability is computed as
\begin{equation}
    R(\nu^*, \nu) = \frac{\pi(\nu^*|D)}{\pi(\nu|D)} = \frac{\exp(-\lambda \tilde\ell) \pi(\nu^*)}{\exp(-\lambda \ell^{(t)})\pi(\nu)}
\end{equation}
The prior $\pi(\nu) = \pi_1(\nu)$ is defined in this case as the Gamma distribution on $Y_i$:
\begin{equation}
    \pi_1(y;\alpha, 1) = \cfrac{1}{\Gamma(\alpha)}y^{\alpha - 1} \exp(-y)
\end{equation}
We can now rewrite $\alpha$ with this definition:
\begin{align}
    R(\nu^*, \nu) = \frac{\exp(-\lambda \tilde\ell) {\tilde Y_i}^{\alpha - 1} \exp(-\tilde Y_i)}{\exp(-\lambda \ell^{(t)}) {Y_i^{(t)}}^{\alpha - 1} \exp(-Y_i^{(t)})}
\end{align}
where we have replaced $y$ by either $\tilde Y_i$ or $Y^{(t)}_i$. Finally, we can apply the log transform:
\begin{align}
    \log(R(\nu^*, \nu)) &= \log\left(\frac{\exp(-\lambda \tilde\ell) {\tilde Y_i}^{\alpha - 1} \exp(-\tilde Y_i)}{\exp(-\lambda \ell^{(t)}) {Y_i^{(t)}}^{\alpha - 1} \exp(-Y_i^{(t)})}\right)\\
    &= \log(\exp(-\lambda \tilde\ell) {\tilde Y_i}^{\alpha - 1} \exp(-\tilde Y_i)) - \log(\exp(-\lambda \ell^{(t)}) {Y_i^{(t)}}^{\alpha - 1} \exp(-Y_i^{(t)}))\\
    &= -\lambda \tilde \ell + (\alpha - 1)\log(\tilde Y_i) - \tilde Y_i - (-\lambda \ell^{(t)} + (\alpha - 1)\log(Y_i^{(t)}) - Y_i^{(t)})\\
    &= -\lambda \tilde \ell + \lambda \ell^{(t)} + (\alpha -1)\log(\frac{\tilde Y_i}{Y_i^{(t)}}) - \tilde Y_i + Y_i^{(t)}
\end{align}

\subsection*{Derivation of $\log(A)$}
The derivation of $\log(A)(\tilde V, V^{(t)}, \gamma^{(t)}) = \log(A(\nu^*, \nu))$ is very similar to the one of $\log(R)$, however much simpler as it does not involve the prior. It is not exactly clear why it is possible and the authors do not provide their derivation, hence we must take guesses. One option is the fact the prior is symmetric (as it is a normal distribution), and in the case of considering it as a proposal, we can simplify the numerator and denominator. Note that this conflicts with what is done for $\log(R)$, as there the prior is considered as part of the posterior. Let's define
\begin{align}
    \tilde \ell &= \ell^{}(\sum_{i=1}^{d} \gamma_i^{(t)} \tilde V_i \tilde V_i^\dagger, \mb D)\\
    \ell^{(t)} &= \ell^{}(\sum_{i=1}^{d} \gamma_i^{(t)} V_i^{(t)} (V_i^{(t)})^\dagger, \mb D)
\end{align}
Then, we can calculate $\log(A)$ as:
\begin{align}
    \log(A(\nu^*, \nu)) &= \log(\frac{\pi(\nu^*) p(\nu^*|\nu)}{\pi(\nu) p(\nu|\nu^*)})\\
                        &= \log(\frac{\exp(-\lambda \tilde \ell)}{\exp(-\lambda \ell^{(t)})})\\
                        &= -\lambda \tilde \ell + \lambda \ell^{(t)}
\end{align}
\includepdf[pages={2}]{cover/cover-danila-mokeev.pdf}
\end{document}
