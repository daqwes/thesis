\documentclass{beamer}
\usetheme{metropolis}
\setbeamercovered{transparent=25} 
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath,oldgerm}
\usefonttheme[onlymath]{serif}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{subcaption}
% \usepackage{float}
% \usepackage{floatrow}
% \usepackage{subfig}
\usepackage{floatrow}

\usepackage[linesnumbered,ruled]{algorithm2e}


\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\addbibresource{bibliography.bib}
% \usepackage{xargs}
% \usepackage[export]{adjustbox}

\newcommand{\greencheck}{\color{green}\checkmark}
\newcommand{\redcross}{\color{red}\times}
\newcommand{\tr}{\text{tr}}

\newcommand{\Tr}{\text{Tr}}

\newcommand{\mb}{\mathbf}

\newcommand{\bsy}{\boldsymbol}

\newcommand{\tb}{\textbf}

\newcommand{\ti}{\textit}

\newcommand{\btheta}{\boldsymbol{\theta}}

\newcommand{\brho}{\boldsymbol{\rho}}

\newcommand{\nmeasn}[1]{$n_{\text{meas}}=#1$}

\newcommand{\nitern}[1]{$n_{\text{iter}}=#1$}

\newcommand{\nburninn}[1]{$n_{\text{burnin}}=#1$}

\newcommand{\rhorankn}[1]{\text{rank}$(\rho)=#1$}

\newcommand{\nmeas}[0]{$n_{\text{meas}} $ }

\newcommand{\niter}[0]{$n_{\text{iter}} $ }

\newcommand{\nburnin}[0]{$n_{\text{burnin}} $ }

\newcommand{\rhorank}[0]{\text{rank}$(\rho) $ }

\newcommand{\semitransp}[2][35]{\textcolor{fg!#1}{#2}}

\usetheme{default}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\title{Master's thesis: Numerical comparison of MCMC methods for Quantum tomography}
\author[Mokeev]
{ Danila Mokeev\\{\small Supervisors: Estelle Massart, Andrew Thompson and Tameem Adel}}
\institute[EPL]{Ecole Polytechnique de Louvain}
\date{21st of June 2024}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\frame{\titlepage}

\begin{frame}{Plan of this thesis}
    \tb{Topic}: Markov chain Monte Carlo (MCMC) methods in Quantum tomography\medbreak
    \tb{Research questions:} 
    \begin{enumerate}
        \item How do these methods perform in different experimental setups?
        \item Why do some methods perform better than others?
    \end{enumerate}
    \tb{Purpose:}
    \begin{itemize}
        \item Enable new directions of research
        \item Help researchers make an informed choice for their use case 
    \end{itemize}
\end{frame}
\begin{frame}{Thesis contributions}
    \begin{enumerate}
        \item Numerically compare 2 MCMC algorithms, the prob-estimator and the Projected Langevin algorithm
        \item Propose 2 new algorithms to understand the impact of the prior and the algorithm on the accuracy
    \end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}

\section{Brief introduction to Quantum tomography}
\begin{frame}{Motivation behind Quantum tomography}
    Quantum tomography is a process to reconstruct the quantum state of a system.\medbreak
    There are some challenges to consider: 
    \begin{itemize}
        \item Quantum systems are inherently probabilistic
        \item A measurement can only be made once
        \item We can only measure the position or momentum, but not both
    \end{itemize}
\end{frame}
\begin{frame}{Quantum tomography: a diagram}
    Quantum tomography allows to address the existing challenges\medbreak %provides a solution to this problem.\medbreak
    % Key steps:
    % \begin{enumerate}
    %     \item Replicate the initial state of the system multiple times
    %     \item Measure each clone once
    %     \item Calculate the empirical probabilities
    %     \item Estimate the quantum state with any appropriate method
    % \end{enumerate}
    \begin{figure}[H]
        %experiments/iterations_no_avg_sep_prob_pl_mhs_mhgs
            \centering
            \includegraphics[width=\textwidth]{figures/diagram_qt_zoom_final.png}
            % \includegraphics[width=0.7\textwidth]{figures/experiments/mhs_mhgs/iters_acc_comp_iters_no_avg_sep_prob_pl_mhs_mhgs-1.png}
        
            % \caption{Convergence plot with the prob-estimator, Projected Langevin, MHS, and MHGS with $n=3$ and separate qubit data generation}
        \end{figure}

\end{frame}

\begin{frame}{Quantum tomography: mathematical description (1)}
    The Born rule states that
    \begin{equation}
        p(m) = \tr(\rho P_m)
    \end{equation}
    with
    \begin{itemize}
        \item $P_m$ the projector matrix associated to the eigenvalue $m$ of an \textit{observable} $O$
        \item $p(m)$ the probability of occurence  of $m$
        \item $\rho$ the \textit{density matrix} representing the quantum state
        \begin{itemize}
            \item positive semi-definite
            \item Hermitian ($\rho = \rho^\dagger$)
            \item trace$(\rho)=1$
            \item size $2^n \times 2^n$ with $n$ the number of qubits
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}{Quantum tomography: mathematical description (2)}
    If we flatten the matrices
    \begin{equation}
        A = \begin{bmatrix}
            \vec P_1\\
            \vec P_2\\
            \vec P_3\\
            \vdots
        \end{bmatrix}
        \quad\vec{\rho} = \begin{bmatrix}
            \rho_{11} \\
            \rho_{12} \\
            \rho_{13} \\
            \vdots
        \end{bmatrix}
    \end{equation}

    then we can estimate $\rho$ by solving the resulting system of equations 
    \begin{equation}
        A\vec{\rho} = \hat p
    \end{equation}
\end{frame}
\begin{frame}{Most common methods}
    \begin{itemize}
        \item Direct methods: \begin{equation}
            \hat \rho = (A^TA)^{-1}A^T \hat p
        \end{equation}
        \item Optimization-based methods: \begin{equation}
            \hat \rho = \text{argmin}_{\vec\rho} ||A \vec\rho - \hat p||
        \end{equation} 
        \item Pauli basis expansion:\begin{equation}
            \hat \rho = \sum_{b\in\{I,x,y,z\}^n} \rho_b \sigma_b
        \end{equation}
        \item Bayesian methods, and in particular MCMC methods
        \begin{equation}
        \hat \rho = \frac{1}{N}\sum_{i=1}^N \rho_i \quad \text{with } \rho_i \sim \pi(\rho|\mb D)
        \end{equation}
    \end{itemize}
\end{frame}
\begin{frame}{Existing methods: our focus in this thesis}
    \begin{itemize}
        % \semitransp{
        \item<0> Direct methods: \begin{equation}
            \hat \rho = (A^TA)^{-1}A^T \hat p
        \end{equation}
        \item<0> Optimization-based methods: \begin{equation}
            \hat \rho = \text{argmin}_{\vec\rho} ||A \vec\rho - \hat p||
        \end{equation} 
        \item<0> Pauli basis expansion:
        \begin{equation}
            \hat \rho = \sum_{b\in\{I,x,y,z\}^n} \rho_b \sigma_b
        \end{equation}
        % }
        \item<1> Bayesian methods, and in particular MCMC methods
        \begin{equation}
        \hat \rho = \frac{1}{N}\sum_{i=1}^N \rho_i \quad \text{with } \rho_i \sim \pi(\rho|\mb D)
        \end{equation}
    \end{itemize}
\end{frame}
\section{Markov chain Monte Carlo methods}
\begin{frame}{Bayesian inference}
    \tb{Context}: We are working in the Bayesian framework
    \begin{equation}
        \underbrace{\pi (\rho|\mb D)}_{\text{Posterior}} \propto \underbrace{\mathcal{L}(\mb D|\rho)}_{\text{Likelihood}} \underbrace{\pi(\rho)}_{\text{Prior}}
    \end{equation}
    Recall that each term is a distribution!\medbreak
    In the context of Quantum tomography:
    \begin{itemize}
        \item Likelihood $\mathcal{L}(\mb D|\rho) = \exp(-||A \vec\rho - \hat p||)$
        \item Prior $\pi(\rho)$ is method specific
        % \item Posterior $\pi (\rho|\mb D)$ corresponds to a distribution over density matrices $\rho$ after being fitted on the data
    \end{itemize}
\end{frame}
\begin{frame}{Markov chain Monte Carlo methods}
    \begin{itemize}
        \item Markov chain Monte Carlo (MCMC) methods \textit{sample} from $\pi (\rho|\mb D)$.
        \item They build a Markov chain of samples $\rho_1, \rho_2, \dots$ such that
        \begin{equation}
            f(x) =\pi (\rho|\mb D)
        \end{equation}
        with the equilibrium distribution $f(x)$ of the chain
        \item The density matrix is then calculated as
        \begin{align}
            \tilde \rho &= \mathbb{E}[\rho]= \int \rho \pi(\rho|\mb D) d\rho\\
            \Leftrightarrow \hat \rho &= \frac{1}{N}\sum_{i=1}^N \rho_i \quad \text{with } \rho_i \sim \pi(\rho|\mb D)
        \end{align}
        % \begin{equation}
            
        % \end{equation}
        % \begin{equation}
        %     \hat \rho = \frac{1}{N}\sum_{i=1}^N \rho_i \quad \text{with } \rho_i \sim \pi(\rho|\mb D)
        % \end{equation}
    \end{itemize}
\end{frame}

% \begin{frame}{The Metropolis-Hastings algorithm}
%     % \begin{itemize}
%     %     \item One of the most common MCMC algorithms
%     % \end{itemize}
%     \begin{algorithm}[H]\label{code:prob}

%         \DontPrintSemicolon
    
%         % \SetKwInOut{Input}{Input}
    
%         % \SetKwInOut{Output}{Output}
    
    
%         % %\underline{Prob-estimator}\;
    
%         % \Input{$d = 2^n, \lambda \in \mathbb{R}, \alpha \in \mathbb{R}, T \in \mathbb{N}, V^{(0)} \in \mathbb{C}^{d \times d}, Y^{(0)} \in \mathbb{C}^{d\times 1}$}
    
%         % \Output{$\hat \rho \in \mathbb{C}^{d\times d}$}
    
%         % $\hat \rho \gets \mb 0$\;
    
%         % $\gamma^{(0)} \in \mathbb{R}^{d\times1} \gets Y^{(0)} /(\sum_{i=1}^d Y_i^{(0)})$\;
    
%         \For{$t\gets 1:T$}{
%             \begin{enumerate}
%                 \item Generate a candidate $\rho^* \sim \overbrace{q(\rho|\rho^{(t-1)})}^{\text{proposal}}$
%                 \item Set $\rho^ {(t)} = \begin{cases}
%                     \rho^{*} \hspace{1.08cm} \text{with prob. } \alpha(\rho^*, \rho^{(t-1)})\\
%                     \rho^{(t-1)} \hspace{0.5cm} \text{with prob. } 1-\alpha(\rho^*, \rho^{(t-1)})
%                 \end{cases}$
%             \end{enumerate}
%             with \begin{equation}
%                 \underbrace{\alpha(\rho^*, \rho^{(t-1)})}_{\text{acceptance ratio}} = \frac{\pi(\rho^*|\mb D) q( \rho^{(t-1)}|\rho^*)}{\pi(\rho^{(t-1)}|\mb D) q( \rho^{*}|\rho^{(t-1)})}
%             \end{equation}
%         }
%         \caption{Metropolis-Hastings algorithm}

%     \end{algorithm}

% \end{frame}

\begin{frame}{An example: Metropolis-Hastings algorithm}
    \href{run:./mcmc.gif}{mcmc.gif}

\end{frame}

\begin{frame}{Advantages of MCMC algorithms}
    Why are we interested in MCMC methods?
    \begin{itemize}
        \item Prior $\pi(\rho)$: additional information about the density matrix - low-rank for example
        \item Uncertainty quantification: working with distributions instead of point estimates
    \end{itemize} 
\end{frame}
\section{Main algorithms}
\begin{frame}{Prob-estimator (1)}
    Introduced in \cite{MA17}, it combines Metropolis-within-Gibbs sampling with a low-rank prior.
    \begin{itemize}
        \item Sum of rank-1 matrices: \[\rho = \sum_{i=1}^{d} \gamma_i V_i {V_i}^\dagger\]
        \item $\pi_1(\gamma_1 \dots \gamma_d)$ is a Dirichlet distribution with a small, constant parameter, leading to sparse values
        \begin{equation*}
            \gamma = \begin{bmatrix}
                0 & \cdots & 1 & \cdots & 0
            \end{bmatrix}
        \end{equation*}
        \item $\pi_2(V_i)$ is a unit sphere distribution
        \begin{equation*}
            ||V_i|| = 1
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Prob-estimator (2)}
    Mix between Metropolis-Hastings and Gibbs sampling\medbreak
    
\begin{algorithm}[H]

    \DontPrintSemicolon

    \For{$t\gets 1:T$}{

            \For{$i\gets 1:d$}{

            \begin{enumerate}
                \item Sample $ \gamma_i^*$ from $\pi_1(\gamma_i)$
                \item Update $\gamma^{(t)}$ with accept/reject step
            \end{enumerate}

            }

            \For{$i\gets 1:d$}{
                \begin{enumerate}
                    \item Sample $V_i^*$ from $\pi_2(V_i)$
                    \item Update $V^{(t)}$ with an accept/reject step
                \end{enumerate}
    
        }
    }

    \caption{Prob-estimator algorithm}

\end{algorithm}\medbreak

\end{frame}
\begin{frame}{Projected Langevin algorithm (1)}
    Introduced in \cite{ACMT2024}, it combines the Unadjusted Langevin algorithm with a \textit{different} low-rank prior.
    \begin{itemize}
        \item Burer-Monteiro factorization: $\rho = YY^\dagger$, with rank$(Y)=r$ 
        \item Low-rank prior: spectral scaled Student-t distribution
        % \begin{equation}
        %     \pi(Y) = C_\theta \det(\theta^2I_d + YY^\dagger)^{-(2d+r+2)/2}
        % \end{equation}
        \begin{equation}
            \pi(Y) = \prod_{j=1}^r (\theta^2 + \smash{\underbrace{s_j(Y)^2}_{j\text{th eigenvalue of } Y}})^{-(2d+r+2)/2}
        \end{equation}
        \item Promotes sparsity among the eigenvalues leading to a low rank
        \item Very similar to the Student-t distribution
        % \begin{itemize}
  
        % \end{itemize}
    \end{itemize} 
\end{frame}

\begin{frame}{Projected Langevin algorithm (2)}

    \begin{algorithm}[H]

        \DontPrintSemicolon
    
        \For{$t\gets 1:T$}{
            % \begin{equation}
            \begin{enumerate}
                \item Sample $\tilde w^{(t)} \sim N(\mb 0, \mb I)$
                \item $\tilde Y^{(t)} \gets \tilde Y^{(t-1)} - \eta^{(t)} \underbrace{\nabla f  (\tilde Y^{(t-1)}, \mb D)}_{\text{gradient}} + \cfrac{\sqrt{2\eta^{(t)}}}{\beta} \tilde w^{(t)}
            $\medbreak with $\pi(Y|\mb D) = \exp(-f(Y, \mb D))$
        \end{enumerate}
        }
    
        \caption{Projected Langevin algorithm}
    
    \end{algorithm}
    Observe that:
    \begin{itemize}
        \item There is no accept/reject step
        \item We use the gradient of the posterior
    \end{itemize}
\end{frame}
\section{Experiments and results}


\begin{frame}{Convergence plot}
    
\begin{figure}[H]
    %experiments/iterations_no_avg_sep_prob_pl_mhs_mhgs
        \centering
        \includegraphics[width=0.7\textwidth]{figures/experiments/baseline/iters_acc_comp_iters_no_avg-1.png}
        % \includegraphics[width=0.7\textwidth]{figures/experiments/mhs_mhgs/iters_acc_comp_iters_no_avg_sep_prob_pl_mhs_mhgs-1.png}
    
        % \caption{Convergence plot with the prob-estimator, Projected Langevin, MHS, and MHGS with $n=3$ and separate qubit data generation}
\end{figure}
$\Longrightarrow$ Projected Langevin converges faster
\end{frame}

\begin{frame}{Convergence across qubits (1)}
    Reminder: $n$ is the number of qubits
    \begin{figure}[H]

        \centering
    
        \begin{subfigure}[b]{0.38\textwidth}
    
            % experiments/iterations_no_avg_rank1
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_iters_no_avg-1.png}
    
            \caption{$n=3$}
        
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.38\textwidth}
    
            % experiments/iterations_no_avg_n4
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_iters_no_avg_n4-1.png}
    
            \caption{$n=4$}
        
        \end{subfigure}
    
        \begin{subfigure}[b]{0.38\textwidth}
    
            % experiments/iterations_no_avg_n5
    
            \centering
            \includegraphics[width=\linewidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_iters_no_avg_n5-1.png}
        \end{subfigure}
            \quad
            \begin{minipage}[b]{.1\textwidth}
            \subcaption{$n=5$}
            \end{minipage}
            \hspace*{\fill}
            \raisebox{10ex}{
            \begin{minipage}{.4\textwidth}
                Projected Langevin converges faster and is more accurate for higher $n$! But..
            \end{minipage}
            }
            % \hspace*{\fill}
    \end{figure}
\end{frame}
    

\begin{frame}{Computation time across qubits (2)}
    
    \begin{figure}[H]

        \centering
    
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_rank1
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_time_no_avg-1.png}
    
            \caption{$n=3$}
    
            \label{fig:conv-plot-diff-n-3-sub}
    
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_n4
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_time_no_avg_n4-1.png}
    
            \caption{$n=4$}
    
            \label{fig:conv-plot-diff-n-4-sub}
    
        \end{subfigure}
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_n5
    
            
            \includegraphics[width=\linewidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_time_no_avg_n5-1.png}
        \end{subfigure}
            \quad
            \begin{minipage}[b]{.1\textwidth}
            \subcaption{$n=5$}
            \end{minipage}
            \hspace*{\fill}
            \raisebox{10ex}{
            \begin{minipage}{.4\textwidth}
                When $n$ increases, the computation time does too!
            \end{minipage}
            }
        \label{fig:conv-plot-diff-n}
    
    \end{figure}
\end{frame}

\begin{frame}{Introducing 2 new methods}
    % Not happy about the slide
    What makes Projected Langevin perform better ?\medbreak
    To answer this question, we introduce 2 new algorithms:
    \begin{enumerate} 
    \item Metropolis-Hastings with Student-t prior (MHS)
    \item Metropolis-Hastings with Gibbs with Student-t prior (MHGS)
    \end{enumerate}
    They combine:
    \begin{itemize}
        \item The algorithm from the prob-estimator
        \item The prior from the Projected Langevin algorithm
    \end{itemize}
\end{frame}

\begin{frame}{Convergence comparison}
    \begin{figure}[H]
        %experiments/iterations_no_avg_sep_prob_pl_mhs_mhgs
            \centering
            \includegraphics[width=0.7\textwidth]{figures/experiments/mhs_mhgs/iters_acc_comp_iters_no_avg_sep_prob_pl_mhs_mhgs-1.png}
            % \includegraphics[width=0.7\textwidth]{figures/experiments/mhs_mhgs/iters_acc_comp_iters_no_avg_sep_prob_pl_mhs_mhgs-1.png}
        
            % \caption{Convergence plot with the prob-estimator, Projected Langevin, MHS, and MHGS with $n=3$ and separate qubit data generation}
        \end{figure}
        $\Longrightarrow$ The prior itself is not a solution, and must be paired with a fast algorithm
\end{frame}

\begin{frame}{Impact of the number of shots}
    Shot: measurement we perform on a clone of the state
    \begin{figure}[H]

        \centering
        \includegraphics[width=0.7\textwidth]{figures/experiments/shots/shots_acc_comp_shots_exp_loglog-1.png}
        % \begin{subfigure}[b]{0.49\textwidth}
    
        %     % experiments/shots
    
        %     \centering
    
        %     \includegraphics[width=\textwidth]{figures/experiments/shots/shots_acc_comp_shots_exp_loglog-1.png}
    
        %     \caption{Mixed qubit}
    
        %     \label{fig:shots-comp-mixed-sub}
    
        % \end{subfigure}
        % \hfill
        % \begin{subfigure}[b]{0.49\textwidth}
    
        %     % experiments/shots_sep
    
        %     \centering
    
        %     \includegraphics[width=\textwidth]{figures/experiments/shots/shots_acc_comp_shots_exp_sep_loglog-1.png}
    
        %     \caption{Separate qubit}
    
        %     \label{fig:shots-comp-sep-sub}
    
        % \end{subfigure}
        
        % \label{fig:shots-comp}
    
    \end{figure}
    $\Longrightarrow$ The prob-estimator does not scale!
\end{frame}

\begin{frame}{Impact of knowing the rank of $\rho$}
    Reminder: for Projected Langevin, $\rho = YY^\dagger$, with rank$(Y)=r$ 
    \begin{figure}[H]

        \centering
    
        \begin{subfigure}[b]{0.49\textwidth}
    
            % experiments/rank_known
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/rank_info/rank_known-1.png}
    
            \caption{Rank of $\rho$ known}
    
            \label{fig:rank-info-sub}
    
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}
    
            % experiments/rank_not_known
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/rank_info/rank_not_known-1.png}
    
            \caption{Rank of $\rho$ not known}
    
            \label{fig:rank-no-info-sub}
    
        \end{subfigure}
        
        \label{fig:rank-info}
    
    \end{figure}
    $\Longrightarrow$ The information about the rank only marginally affects the accuracy
\end{frame}

\begin{frame}{Summary and future work}
    \begin{itemize}
        \item Quantum tomography is not yet a solved problem, especially for large  systems
        \item MCMC methods are a promising direction of research, thanks to uncertainty quantification and prior information 
        \item The choice of the algorithm might have more impact on the scalability of a method than the prior
        \item More experiments are needed to investigate the performance and scalability (for example with other gradient-based methods and priors)
    \end{itemize}
    % - The Projected Langevin algorithm requires less iterations and is more accurate for larger $n$ than the prob-estimator, however it does not scale in terms of computation time
    % - Its convergence speed and accuracy are due to a combination of prior \textit{algorithm}, not just the prior
\end{frame}
\begin{frame}{References}
    \printbibliography
\end{frame}
\end{document}

