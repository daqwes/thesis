\documentclass{beamer}
\usetheme{metropolis}
\setbeamercovered{transparent=25} 
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath,oldgerm}
\usefonttheme[onlymath]{serif}
\usepackage{tikz}
\usepackage{xcolor}

\usepackage{subcaption}
% \usepackage{float}
% \usepackage{floatrow}
% \usepackage{subfig}
\usepackage{floatrow}

\usepackage[linesnumbered,ruled]{algorithm2e}


\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\addbibresource{bibliography.bib}
% \usepackage{xargs}
% \usepackage[export]{adjustbox}

\newcommand{\greencheck}{\color{green}\checkmark}
\newcommand{\redcross}{\color{red}\times}
\newcommand{\tr}{\text{tr}}

\newcommand{\Tr}{\text{Tr}}

\newcommand{\mb}{\mathbf}

\newcommand{\bsy}{\boldsymbol}

\newcommand{\tb}{\textbf}

\newcommand{\ti}{\textit}

\newcommand{\btheta}{\boldsymbol{\theta}}

\newcommand{\brho}{\boldsymbol{\rho}}

\newcommand{\nmeasn}[1]{$n_{\text{meas}}=#1$}

\newcommand{\nitern}[1]{$n_{\text{iter}}=#1$}

\newcommand{\nburninn}[1]{$n_{\text{burnin}}=#1$}

\newcommand{\rhorankn}[1]{\text{rank}$(\rho)=#1$}

\newcommand{\nmeas}[0]{$n_{\text{meas}} $ }

\newcommand{\niter}[0]{$n_{\text{iter}} $ }

\newcommand{\nburnin}[0]{$n_{\text{burnin}} $ }

\newcommand{\rhorank}[0]{\text{rank}$(\rho) $ }

\newcommand{\semitransp}[2][35]{\textcolor{fg!#1}{#2}}

\usetheme{default}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\title{Master's thesis: Numerical comparison of MCMC methods for Quantum Tomography}
\author[Mokeev]
{ Danila Mokeev\\{\small Supervisors: Estelle Massart and Andrew Thompson}}
\institute[EPL]{Ecole Polytechnique de Louvain}
\date{21st of June 2024}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\frame{\titlepage}

\begin{frame}{Plan of this thesis}
    \begin{itemize}
        \item This thesis main problem is Quantum Tomography
        \item We talk about algorithms that solve that problem, in particular MCMC
        \item In our experiments, we numerically compare 2 methods in different experimental setups
        \item We also introduce the 2 new algorithms to understand why one algorithm might work better than the other 
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}

\section{Brief introduction to Quantum tomography}
\begin{frame}{Problem: quantum state reconstruction}
    \tb{Goal}: Reconstitute a quantum state\medbreak
    Unfortunately, there are some challenges: 
    \begin{itemize}
        \item Quantum systems are inherently probabilistic
        \item A measurement can ony be made once
        \item We can only measure the position or momentum, but not both
    \end{itemize}
\end{frame}
\begin{frame}{Quantum Tomography}
    Quantum tomography provides a solution to this problem.\medbreak
    Key steps:
    \begin{enumerate}
        \item Replicate the initial state of the system multiple times
        \item Measure each clone once
        \item Calculate the empirical probabilities
        \item Estimate the quantum state with any appropriate method
    \end{enumerate}

\end{frame}

\begin{frame}{Quantum Tomography: mathematical description (1)}
    The Born rule states that
    \begin{equation}
        p(m) = \tr(\rho P_m)
    \end{equation}
    with
    \begin{itemize}
        \item $P_m$ the projector matrix associated to the eigenvalue $m$ of an \textit{observable} $O$
        \item $p(m)$ the probability of occurence  of $m$
        \item $\rho$ the \textit{density matrix} representing the quantum state
        \begin{itemize}
            \item positive semi-definite
            \item Hermitian ($\rho = \rho^\dagger$)
            \item trace$(\rho)=1$
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}{Quantum Tomography: mathematical description (2)}
    If we flatten the matrices
    \begin{equation}
        A = \begin{bmatrix}
            \vec P_1\\
            \vec P_2\\
            \vec P_3\\
            \vdots
        \end{bmatrix}
        \quad\vec{\rho} = \begin{bmatrix}
            \rho_{11} \\
            \rho_{12} \\
            \rho_{13} \\
            \vdots
        \end{bmatrix}
    \end{equation}

    then we can estimate $\rho$ by solving the resulting system of equations 
    \begin{equation}
        A\vec{\rho} = \hat p
    \end{equation}
\end{frame}
\begin{frame}{Existing methods}
    \begin{itemize}
        \item Direct methods: \begin{equation}
            \hat \rho = (A^TA)^{-1}A^T \hat p
        \end{equation}
        \item Optimization-based methods: \begin{equation}
            \hat \rho = \text{argmin}_{\vec\rho} ||A \vec\rho - \hat p||
        \end{equation} 
        \item Pauli basis expansion:\begin{equation}
            \hat \rho = \sum_{b\in\{I,x,y,z\}^n} \rho_b \sigma_b
        \end{equation}
        \item Bayesian methods, and in particular MCMC methods
        \begin{equation}
        \hat \rho = \frac{1}{N}\sum_{i=1}^N \rho_i \quad \text{with } \rho_i \sim \pi(\rho|\mb D)
        \end{equation}
    \end{itemize}
\end{frame}
\begin{frame}{Existing methods: our focus in this thesis}
    \begin{itemize}
        % \semitransp{
        \item<0> Direct methods: \begin{equation}
            \hat \rho = (A^TA)^{-1}A^T \hat p
        \end{equation}
        \item<0> Optimization-based methods: \begin{equation}
            \hat \rho = \text{argmin}_{\vec\rho} ||A \vec\rho - \hat p||
        \end{equation} 
        \item<0> Pauli basis expansion:
        \begin{equation}
            \hat \rho = \sum_{b\in\{I,x,y,z\}^n} \rho_b \sigma_b
        \end{equation}
        % }
        \item<1> Bayesian methods, and in particular MCMC methods
        \begin{equation}
        \hat \rho = \frac{1}{N}\sum_{i=1}^N \rho_i \quad \text{with } \rho_i \sim \pi(\rho|\mb D)
        \end{equation}
    \end{itemize}
\end{frame}
\section{Markov chain Monte Carlo methods}
\begin{frame}{Bayesian inference}
    \tb{Context}: We are working in the Bayesian framework:
    \begin{equation}
        \pi (\rho|\mb D) \propto \mathcal{L}(\mb D|\rho) \pi(\rho)
    \end{equation}
    In the context of Quantum Tomography:
    \begin{itemize}
        \item Likelihood $\mathcal{L}(\mb D|\rho) = ||A \vec\rho - \hat p||$
        \item Prior $\pi(\rho)$ is method specific
        \item Posterior $\pi (\rho|\mb D)$ corresponds to a distribution over density matrices $\rho$ 
    \end{itemize}
\end{frame}
\begin{frame}{Markov chain Monte Carlo methods}
    \begin{itemize}
        \item Markov chain Monte Carlo (MCMC) methods \textit{sample} from $\pi (\rho|\mb D)$.
        \item They build a Markov chain of samples $\rho_1, \rho_2, \dots$ such that
        \begin{equation}
            f(x) =\pi (\rho|\mb D)
        \end{equation}
        with the equilibrium distribution $f(x)$ of the chain
        \item The density matrix is then approximated as 
        \begin{equation}
            \hat \rho = \frac{1}{N}\sum_{i=1}^N \rho_i \quad \text{with } \rho_i \sim \pi(\rho|\mb D)
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}{The Metropolis-Hastings algorithm}
    % \begin{itemize}
    %     \item One of the most common MCMC algorithms
    % \end{itemize}
    \begin{algorithm}[H]\label{code:prob}

        \DontPrintSemicolon
    
        % \SetKwInOut{Input}{Input}
    
        % \SetKwInOut{Output}{Output}
    
    
        % %\underline{Prob-estimator}\;
    
        % \Input{$d = 2^n, \lambda \in \mathbb{R}, \alpha \in \mathbb{R}, T \in \mathbb{N}, V^{(0)} \in \mathbb{C}^{d \times d}, Y^{(0)} \in \mathbb{C}^{d\times 1}$}
    
        % \Output{$\hat \rho \in \mathbb{C}^{d\times d}$}
    
        % $\hat \rho \gets \mb 0$\;
    
        % $\gamma^{(0)} \in \mathbb{R}^{d\times1} \gets Y^{(0)} /(\sum_{i=1}^d Y_i^{(0)})$\;
    
        \For{$t\gets 1:T$}{
            \begin{enumerate}
                \item Generate a candidate $\rho^* \sim \overbrace{q(\rho|\rho^{(t-1)})}^{\text{proposal}}$
                \item Set $\rho^ {(t)} = \begin{cases}
                    \rho^{*} \hspace{1.08cm} \text{with prob. } \alpha(\rho^*, \rho^{(t-1)})\\
                    \rho^{(t-1)} \hspace{0.5cm} \text{with prob. } 1-\alpha(\rho^*, \rho^{(t-1)})
                \end{cases}$
            \end{enumerate}
            with \begin{equation}
                \underbrace{\alpha(\rho^*, \rho^{(t-1)})}_{\text{acceptance ratio}} = \frac{\pi(\rho^*|\mb D) q( \rho^{(t-1)}|\rho^*)}{\pi(\rho^{(t-1)}|\mb D) q( \rho^{*}|\rho^{(t-1)})}
            \end{equation}
        }
        \caption{Metropolis-Hastings algorithm}

    \end{algorithm}

\end{frame}

\begin{frame}{Illustration of the Metropolis-Hastings algorithm}
    \href{run:./mcmc.gif}{mcmc.gif}

\end{frame}

\begin{frame}{Advantages of MCMC algorithms}
    Why are we interested in MCMC methods?
    \begin{itemize}
        \item Prior $\pi(\rho)$: additional information about the density matrix - low-rank for example
        \item Uncertainty quantification: working with distributions instead of point estimates
    \end{itemize} 
\end{frame}

\begin{frame}{Prob-estimator (1)}
    Introduced in \cite{MA17}, it combines Metropolis-within-Gibbs sampling with a low-rank prior.
    \begin{itemize}
        \item Analogous to eigenvector factorization: $\rho = \sum_{i=1}^{d} \gamma_i V_i V_i^\dagger$
        \item $\pi_1(\gamma_1 \dots \gamma_d)$ is a Dirichlet distribution with a small, constant parameter, leading to sparse values 
        \item $\pi_2(V_1 \dots V_d)$ is a unit sphere distribution
    \end{itemize}
\end{frame}

\begin{frame}{Prob-estimator (2)}
    Algorithm: combination between Metropolis-Hastings and Gibbs sampling\medbreak
    
\begin{algorithm}[H]

    \DontPrintSemicolon

    \For{$t\gets 1:T$}{

            \For{$i\gets 1:d$}{

            \begin{enumerate}
                \item Sample $ \gamma_i^*$ from $\pi_1(\gamma_1, \dots, \gamma_d)$
                \item Update $\gamma^{(t)}$ with accept/reject step
            \end{enumerate}

            }

            \For{$i\gets 1:d$}{
                \begin{enumerate}
                    \item Sample $V_i^*$ from $\pi_2(V_1, \dots, V_d)$
                    \item Update $V^{(t)}$ with an accept/reject step
                \end{enumerate}
    
        }
    }

    \caption{Prob-estimator algorithm}

\end{algorithm}\medbreak

\end{frame}
\begin{frame}{Projected Langevin algorithm (1)}
    Introduced in \cite{ACMT2024}, it combines the Unadjusted Langevin algorithm with a different low-rank prior.
    \begin{itemize}
        \item Burer-Monteiro factorization: $\rho = YY^\dagger$, with rank$(Y)=r$ 
        \item Low-rank prior: spectral scaled Student-t distribution
        \begin{equation}
            \pi(Y) = C_\theta \det(\theta^2I_d + YY^\dagger)^{-(2d+r+2)/2}
        \end{equation}
        equivalent to 
        \begin{equation}
            \pi(Y) = \prod_{j=1}^r (\theta^2 + s_j(Y)^2)^{-(2d+r+2)/2}
        \end{equation}
        with $s_j$ the $j$th largest eigenvalue
    \end{itemize} 
\end{frame}

\begin{frame}{Projected Langevin algorithm (2)}
    Note that there is no accept/reject step!

    \begin{algorithm}[H]

        \DontPrintSemicolon
    
        \For{$t\gets 1:T$}{
            % \begin{equation}
            \begin{enumerate}
                \item Sample $\tilde w^{(t)} \sim N(\mb 0, \mb I)$
                \item $\tilde Y^{(t)} \gets \tilde Y^{(t-1)} - \eta^{(t)} \nabla f  (\tilde Y^{(t-1)}, \mb D) + \cfrac{\sqrt{2\eta^{(t)}}}{\beta} \tilde w^{(t)}
            $\medbreak with $\pi(Y|\mb D) = \exp(-f(Y, \mb D))$
        \end{enumerate}
        }
    
        \caption{Projected Langevin algorithm}
    
    \end{algorithm}
\end{frame}
\section{Thesis contributions}

\begin{frame}{Thesis contributions}
    \begin{enumerate}
        \item Numerically compare the prob-estimator and the Projected Langevin algorithm
        \item Propose 2 new algorithms to understand the impact of the prior vs the algorithm on the accuracy
    \end{enumerate}
\end{frame}

\begin{frame}{Numerical comparison: convergence}
    
\begin{figure}[H]
    %experiments/iterations_no_avg_sep_prob_pl_mhs_mhgs
        \centering
        \includegraphics[width=0.7\textwidth]{figures/experiments/baseline/iters_acc_comp_iters_no_avg-1.png}
        % \includegraphics[width=0.7\textwidth]{figures/experiments/mhs_mhgs/iters_acc_comp_iters_no_avg_sep_prob_pl_mhs_mhgs-1.png}
    
        % \caption{Convergence plot with the prob-estimator, Projected Langevin, MHS, and MHGS with $n=3$ and separate qubit data generation}
    \end{figure}
    $\Longrightarrow$ Projected Langevin converges faster
\end{frame}

\begin{frame}{Numerical comparison: convergence across qubits (1)}
    
    \begin{figure}[H]

        \centering
    
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_rank1
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_iters_no_avg-1.png}
    
            \caption{$n=3$}
        
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_n4
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_iters_no_avg_n4-1.png}
    
            \caption{$n=4$}
        
        \end{subfigure}
    
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_n5
    
            \centering
            \includegraphics[width=\linewidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_iters_no_avg_n5-1.png}
        \end{subfigure}
            \quad
            \begin{minipage}[b]{.1\textwidth}
            \subcaption{$n=5$}
            \end{minipage}
        
    \end{figure}
\end{frame}
    

\begin{frame}{Numerical comparison: computation time across qubits (2)}
    
    \begin{figure}[H]

        \centering
    
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_rank1
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_time_no_avg-1.png}
    
            \caption{$n=3$}
    
            \label{fig:conv-plot-diff-n-3-sub}
    
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_n4
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_time_no_avg_n4-1.png}
    
            \caption{$n=4$}
    
            \label{fig:conv-plot-diff-n-4-sub}
    
        \end{subfigure}
    
        \begin{subfigure}[b]{0.4\textwidth}
    
            % experiments/iterations_no_avg_n5
    
            \centering
            \includegraphics[width=\linewidth]{figures/experiments/baseline/diff_n_qubits/iters_acc_comp_time_no_avg_n5-1.png}
        \end{subfigure}
            \quad
            \begin{minipage}[b]{.1\textwidth}
            \subcaption{$n=5$}
            \end{minipage}
    
        \label{fig:conv-plot-diff-n}
    
    \end{figure}
\end{frame}
\begin{frame}{Numerical comparison: number of shots}
    Shot: measurement we perform on a clone of the state
    \begin{figure}[H]

        \centering
    
        \begin{subfigure}[b]{0.49\textwidth}
    
            % experiments/shots
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/shots/shots_acc_comp_shots_exp_loglog-1.png}
    
            \caption{Mixed qubit}
    
            \label{fig:shots-comp-mixed-sub}
    
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}
    
            % experiments/shots_sep
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/shots/shots_acc_comp_shots_exp_sep_loglog-1.png}
    
            \caption{Separate qubit}
    
            \label{fig:shots-comp-sep-sub}
    
        \end{subfigure}
        
        \label{fig:shots-comp}
    
    \end{figure}
    $\Longrightarrow$ The prob-estimator does not scale!
\end{frame}

\begin{frame}{Numerical experiments: impact of knowledge of rank}
    \begin{figure}[H]

        \centering
    
        \begin{subfigure}[b]{0.49\textwidth}
    
            % experiments/rank_known
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/rank_info/rank_known-1.png}
    
            \caption{Rank of $\rho$ known}
    
            \label{fig:rank-info-sub}
    
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}
    
            % experiments/rank_not_known
    
            \centering
    
            \includegraphics[width=\textwidth]{figures/experiments/rank_info/rank_not_known-1.png}
    
            \caption{Rank of $\rho$ not known}
    
            \label{fig:rank-no-info-sub}
    
        \end{subfigure}
    
        \caption{Rank knowledge plot for $n=3$}
    
        \label{fig:rank-info}
    
    \end{figure}
    $\Longrightarrow$ For Projected Langevin, the information about the rank only marginally affects the accuracy
\end{frame}
\begin{frame}
    
\end{frame}
\begin{frame}{References}
    \printbibliography
\end{frame}
\end{document}

